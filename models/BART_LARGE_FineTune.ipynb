{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c6bdd9-c975-4be5-8f75-e45b89c976f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tobysavage/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/tobysavage/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9abf9133-cd20-4259-8004-96b42146b8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_final_csv\n",
      "train.json\n",
      "ambignq.zip\n",
      "dev_final_csv\n",
      "dev.json\n",
      "LICENSE\n",
      "train_final_csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directory = '../../data/LARGE'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if os.path.isfile(os.path.join(directory, filename)):\n",
    "        print(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e19cd18f-3cf3-416c-86a9-50bd8ad3d8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length:  9216\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ambiguous_question</th>\n",
       "      <th>disambiguated_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who sang island in the sun in aquamarine?</td>\n",
       "      <td>Who from the cast sings  \"Island in the Sun\" i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who sang island in the sun in aquamarine?</td>\n",
       "      <td>Who sings \"Island in the Sun\" that is featured...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was the first sign that mt. pinatubo had ...</td>\n",
       "      <td>What was the first sign that mt. pinatubo had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What was the first sign that mt. pinatubo had ...</td>\n",
       "      <td>What was the first sign that mt. pinatubo had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What was the first sign that mt. pinatubo had ...</td>\n",
       "      <td>What was the first sign that mt. pinatubo had ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ambiguous_question  \\\n",
       "0          Who sang island in the sun in aquamarine?   \n",
       "1          Who sang island in the sun in aquamarine?   \n",
       "2  What was the first sign that mt. pinatubo had ...   \n",
       "3  What was the first sign that mt. pinatubo had ...   \n",
       "4  What was the first sign that mt. pinatubo had ...   \n",
       "\n",
       "                              disambiguated_question  \n",
       "0  Who from the cast sings  \"Island in the Sun\" i...  \n",
       "1  Who sings \"Island in the Sun\" that is featured...  \n",
       "2  What was the first sign that mt. pinatubo had ...  \n",
       "3  What was the first sign that mt. pinatubo had ...  \n",
       "4  What was the first sign that mt. pinatubo had ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../../data/LARGE/train_final_csv')\n",
    "df_train = df_train.drop(columns='viewed_doc_titles')\n",
    "df_train = df_train.drop(columns='question_type')\n",
    "print(\"Train Length: \", len(df_train))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09782b75-3599-4361-ab7c-7bcd5eaec95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Length:  4866\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ambiguous_question</th>\n",
       "      <th>disambiguated_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who plays the queen in chronicles of narnia?</td>\n",
       "      <td>Who plays Queen Jadis in Chronicles of Narnia?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who plays the queen in chronicles of narnia?</td>\n",
       "      <td>Who plays Queen Lilliandil in Chronicles of Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many championships do new york knicks have?</td>\n",
       "      <td>How many NBA championships do new york knicks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many championships do new york knicks have?</td>\n",
       "      <td>How many Eastern Conference/Division champions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How far is vidor tx from beaumont tx?</td>\n",
       "      <td>How far is Vidor TX from Beaumont TX by car?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ambiguous_question  \\\n",
       "0     Who plays the queen in chronicles of narnia?   \n",
       "1     Who plays the queen in chronicles of narnia?   \n",
       "2  How many championships do new york knicks have?   \n",
       "3  How many championships do new york knicks have?   \n",
       "4            How far is vidor tx from beaumont tx?   \n",
       "\n",
       "                              disambiguated_question  \n",
       "0     Who plays Queen Jadis in Chronicles of Narnia?  \n",
       "1  Who plays Queen Lilliandil in Chronicles of Na...  \n",
       "2  How many NBA championships do new york knicks ...  \n",
       "3  How many Eastern Conference/Division champions...  \n",
       "4       How far is Vidor TX from Beaumont TX by car?  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('../../data/LARGE/test_final_csv')\n",
    "df_test = df_test.drop(columns='viewed_doc_titles')\n",
    "df_test = df_test.drop(columns='question_type')\n",
    "print(\"Test Length: \", len(df_test))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eab77a0b-fc94-4445-ba8b-a143a645c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train)+len(df_test)==14082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25e67259-b089-466a-8856-2ffabfe02425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Length:  4856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ambiguous_question</th>\n",
       "      <th>disambiguated_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why did the st louis cardinals move to arizona?</td>\n",
       "      <td>What ability caused the st louis cardinals mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why did the st louis cardinals move to arizona?</td>\n",
       "      <td>What physical issue caused the st louis cardin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why did the st louis cardinals move to arizona?</td>\n",
       "      <td>What fan issue caused the st louis cardinals m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is the current chairman of african union c...</td>\n",
       "      <td>Who is the 4th chairman of african union commi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is the current chairman of african union c...</td>\n",
       "      <td>Who is the 3rd chairman of african union commi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ambiguous_question  \\\n",
       "0    Why did the st louis cardinals move to arizona?   \n",
       "1    Why did the st louis cardinals move to arizona?   \n",
       "2    Why did the st louis cardinals move to arizona?   \n",
       "3  Who is the current chairman of african union c...   \n",
       "4  Who is the current chairman of african union c...   \n",
       "\n",
       "                              disambiguated_question  \n",
       "0  What ability caused the st louis cardinals mov...  \n",
       "1  What physical issue caused the st louis cardin...  \n",
       "2  What fan issue caused the st louis cardinals m...  \n",
       "3  Who is the 4th chairman of african union commi...  \n",
       "4  Who is the 3rd chairman of african union commi...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = pd.read_csv('../../data/LARGE/dev_final_csv')\n",
    "print(\"Dev Length: \", len(df_dev))\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9906cfee-0cb3-489d-91b4-f358511ac706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CITATION: HuggingFace on how to fine-tune a model (https://huggingface.co/docs/transformers/training)\n",
    "# Also used help from previous homework assignments from NNDL and ADL of this semester.\n",
    "# Help from ChatGPT on data type/syntax on specific data values\n",
    "# This goes for the initialization of the tokenizer, model, and training loop.\n",
    "\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "# Load pre-trained model\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "model.config.dropout = 0.1\n",
    "\n",
    "# move the model to the GPU\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7278a960-35d1-4bd3-8e12-5f7f20f50bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = list(df_train['ambiguous_question'])\n",
    "targets_train = list(df_train['disambiguated_question'])\n",
    "\n",
    "# Tokenize inputs and targets\n",
    "inputs_train = tokenizer(inputs_train, padding=True, truncation=True, return_tensors='pt')\n",
    "targets_train = tokenizer(targets_train, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bae1533a-739c-423b-a500-e9a06d2d9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITATION: Class produced by ChatGPT to process specific data into dataloader.\n",
    "class AmbigNQDataset(Dataset):\n",
    "    def __init__(self, encodings, resultings):\n",
    "        self.encodings = encodings\n",
    "        self.resultings = resultings\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.resultings['input_ids'][idx]\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.resultings['input_ids'])\n",
    "    \n",
    "dataset_train = AmbigNQDataset(inputs_train, targets_train)\n",
    "loader_train = DataLoader(dataset_train, batch_size = 10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6df2e945-4c98-4857-9204-f0687620d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITATION: Inspiration from  NNDL and ADL coursework on formatting\n",
    "\n",
    "inputs_dev = list(df_dev['ambiguous_question'])\n",
    "targets_dev = list(df_dev['disambiguated_question'])\n",
    "\n",
    "inputs_dev = tokenizer(inputs_dev, padding=True, truncation=True, return_tensors='pt')\n",
    "targets_dev = tokenizer(targets_dev, padding=True, truncation=True, return_tensors='pt')\n",
    "dataset_dev = AmbigNQDataset(inputs_dev, targets_dev)\n",
    "loader_dev = DataLoader(dataset_dev, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cea7ee22-c2d4-4962-92fe-8271a0770fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITATION: Inspiration from  NNDL and ADL coursework on formatting\n",
    "\n",
    "inputs_test = list(df_test['ambiguous_question'])\n",
    "targets_test = list(df_test['disambiguated_question'])\n",
    "\n",
    "inputs_test = tokenizer(inputs_test, padding=True, truncation=True, return_tensors='pt')\n",
    "targets_test = tokenizer(targets_test, padding=True, truncation=True, return_tensors='pt')\n",
    "dataset_test = AmbigNQDataset(inputs_test, targets_test)\n",
    "loader_test = DataLoader(dataset_test, batch_size = 10, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba26bf5-6319-410c-a25b-a5c32c67068e",
   "metadata": {},
   "source": [
    "# Fine-tuning BART-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65750afd-fd8c-45f1-875d-9a0dfabf5cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a167ddef-1a91-4b96-a1f9-019904819fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 1.8261441015789928\n",
      "Epoch: 0, Validation Loss: 0.2675297122923926\n",
      "Epoch: 1, Training Loss: 0.266465969060481\n",
      "Epoch: 1, Validation Loss: 0.2516861463525168\n",
      "Epoch: 2, Training Loss: 0.22710071321487943\n",
      "Epoch: 2, Validation Loss: 0.24580122366424942\n",
      "Epoch: 3, Training Loss: 0.20067720467511588\n",
      "Epoch: 3, Validation Loss: 0.24744277234369344\n",
      "Epoch: 4, Training Loss: 0.17876775348619628\n",
      "Epoch: 4, Validation Loss: 0.2564451183679173\n",
      "Stopping early due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# CITATION: HuggingFace on how to fine-tune a model (https://huggingface.co/docs/transformers/training)\n",
    "# Also used help from previous homework assignments from NNDL and ADL of this semester.\n",
    "# Help from ChatGPT on data type/syntax on specific data values\n",
    "# This goes for the initialization of the tokenizer, model, and training loop.\n",
    "\n",
    "# load pre-trained BART-Large model for fine-tuning\n",
    "epochs = 5\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "\n",
    "# Early Stopping Parameters\n",
    "patience = 2\n",
    "min_delta = 0.001\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# training loop\n",
    "model.train() # put model in train mode\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    for batch in loader_train:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        resultings = batch['labels'].to('cuda')\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=resultings)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss +=loss.item()\n",
    "        \n",
    "    avg_train_loss = total_train_loss / len(loader_train)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch: {epoch}, Training Loss: {avg_train_loss}\")\n",
    "    \n",
    "    \n",
    "    # Validation Step\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader_dev:\n",
    "            input_ids = batch['input_ids'].to('cuda')\n",
    "            attention_mask = batch['attention_mask'].to('cuda')\n",
    "            labels = batch['labels'].to('cuda')\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "    avg_val_loss = total_val_loss/len(loader_dev)\n",
    "    validation_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch: {epoch}, Validation Loss: {avg_val_loss}\")\n",
    "    \n",
    "    if avg_val_loss + min_delta < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # CITATION: Early Stopping with a patients counter implemented suggested from ChatGPT\n",
    "        patience_counter = 0  # reset counter if improvement is found\n",
    "    else:\n",
    "        patience_counter += 1  # increment counter if no improvement\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Stopping early due to no improvement in validation loss.\")\n",
    "        break\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb7f0d9b-da72-4d5b-80a9-3e2e2b94b1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cpu_training_loss = [loss_item.item() for loss_item in training_losses]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9601fa3-3f37-48e8-b2e7-2b9f407b510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITATION: HuggingFace on how to fine-tune a model (https://huggingface.co/docs/transformers/training)\n",
    "# Also used help from previous homework assignments from NNDL and ADL of this semester.\n",
    "# Help from ChatGPT on data type/syntax on specific data values\n",
    "# This goes for the initialization of the tokenizer, model, and training loop.\n",
    "\n",
    "model.eval()  # make sure the model is in evaluation mode\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader_test:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Decode predictions\n",
    "        pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        # print(\"BATCH: \", batch)\n",
    "        # print(\"PREDS: \", pred_texts)\n",
    "        predictions.extend(pred_texts)\n",
    "        \n",
    "        ref_texts = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "        \n",
    "        input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # for i in range(len(pred_texts)):\n",
    "        #     print(f\"INPUT: {input_texts[i]}\")\n",
    "        #     print(f\"ACTUAL: {ref_texts[i]}\")\n",
    "        #     print(f\"PREDICTION: {pred_texts[i]}\")\n",
    "        #     print(\"\\n\")\n",
    "        \n",
    "        references.extend([[ref] for ref in ref_texts])\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18a2482a-33d3-400b-9f50-b09947e4d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus BLEU Score on Test Set: 0.38033264463139393\n"
     ]
    }
   ],
   "source": [
    "# CITATION: Inspiration from NLTK (https://www.nltk.org/_modules/nltk/translate/bleu_score.html)\n",
    "tokenized_predictions = [pred.split() for pred in predictions]\n",
    "tokenized_references = [[ref[0].split()] for ref in references]\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = corpus_bleu(tokenized_references, tokenized_predictions)\n",
    "\n",
    "# Calculate METEOR score\n",
    "# meteor_scores = [meteor_score([ref[0]], pred) for ref, pred in zip(references, predictions)]\n",
    "# average_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "\n",
    "print(f\"Corpus BLEU Score on Test Set: {bleu_score}\")\n",
    "# print(f\"Average METEOR Score on Test Set: {average_meteor_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3e578e7-9b49-46f8-9f45-4a5b55caffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average METEOR Score on Test Set: 0.6238789965717016\n"
     ]
    }
   ],
   "source": [
    "# CITATION: Inspiration from NLTK (https://www.nltk.org/api/nltk.translate.meteor_score.html)\n",
    "\n",
    "# Tokenize the predictions for METEOR score calculation\n",
    "tokenized_predictions_meteor = [pred.split() for pred in predictions]\n",
    "\n",
    "# Check the structure of your references\n",
    "# If each item in references is a list with a single string\n",
    "tokenized_references_meteor = [[ref[0].split()] if len(ref) > 0 else [[]] for ref in references]\n",
    "\n",
    "# Calculate METEOR score\n",
    "meteor_scores = [meteor_score(refs, pred) for refs, pred in zip(tokenized_references_meteor, tokenized_predictions_meteor)]\n",
    "average_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "\n",
    "print(f\"Average METEOR Score on Test Set: {average_meteor_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ccd698c-9f85-4bd3-8d0d-0ebdd2141d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('final_pretrained_saves/tokenizer_config.json',\n",
       " 'final_pretrained_saves/special_tokens_map.json',\n",
       " 'final_pretrained_saves/vocab.json',\n",
       " 'final_pretrained_saves/merges.txt',\n",
       " 'final_pretrained_saves/added_tokens.json')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('final_pretrained_saves')\n",
    "tokenizer.save_pretrained('final_pretrained_saves')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5156e331-1673-4ce1-be03-af1daad853cf",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b119f81-ac7a-4162-a299-9f8d6e662728",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('final_pretrained_saves').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "edcdb6e1-593c-4957-8ee9-e5779ae41764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: Who plays the queen in chronicles of narnia?\n",
      "ACTUAL: Who plays Queen Jadis in Chronicles of Narnia?\n",
      "PREDICTION: Who plays the queen in chronicles of narnia as adult?\n",
      "\n",
      "\n",
      "INPUT: Who plays the queen in chronicles of narnia?\n",
      "ACTUAL: Who plays Queen Lilliandil in Chronicles of Narnia?\n",
      "PREDICTION: Who plays the queen in chronicles of narnia as adult?\n",
      "\n",
      "\n",
      "INPUT: How many championships do new york knicks have?\n",
      "ACTUAL: How many NBA championships do new york knicks have?\n",
      "PREDICTION: How many NBA championships do new york knicks have?\n",
      "\n",
      "\n",
      "INPUT: How many championships do new york knicks have?\n",
      "ACTUAL: How many Eastern Conference/Division championships do new york knicks have?\n",
      "PREDICTION: How many NBA championships do new york knicks have?\n",
      "\n",
      "\n",
      "INPUT: How far is vidor tx from beaumont tx?\n",
      "ACTUAL: How far is Vidor TX from Beaumont TX by car?\n",
      "PREDICTION: How far is vidor tx from beaumont tx in miles?\n",
      "\n",
      "\n",
      "INPUT: How far is vidor tx from beaumont tx?\n",
      "ACTUAL: How far is Vidor TX from Beaumont TX in a southwest direction?\n",
      "PREDICTION: How far is vidor tx from beaumont tx in miles?\n",
      "\n",
      "\n",
      "INPUT: Who is the indian cricket team captain 2018?\n",
      "ACTUAL: Who was the Indian Test Match captain in 2018?\n",
      "PREDICTION: Who is the indian cricket team captain in 2018?\n",
      "\n",
      "\n",
      "INPUT: Who is the indian cricket team captain 2018?\n",
      "ACTUAL: Who was the India one day international captain?\n",
      "PREDICTION: Who is the indian cricket team captain in 2018?\n",
      "\n",
      "\n",
      "INPUT: Who is the highest paid actor in big bang theory?\n",
      "ACTUAL: Based on show salary, who is the highest paid actor on Big Bang Theory?\n",
      "PREDICTION: Who is the highest paid actor in big bang theory as of 2017?\n",
      "\n",
      "\n",
      "INPUT: Who is the highest paid actor in big bang theory?\n",
      "ACTUAL: Including all income sources, who is the highest paid actor on Big Bang Theory?\n",
      "PREDICTION: Who is the highest paid actor in big bang theory as of 2017?\n",
      "\n",
      "\n",
      "INPUT: Which person or group brought polyphonic music into the christian church?\n",
      "ACTUAL: Who wrote the oldest known polyphonic setting of the mass attributable to one composer and brought it into the Christian church?\n",
      "PREDICTION: Which person or group brought polyphonic music into the christian church through\n",
      "\n",
      "\n",
      "INPUT: Which person or group brought polyphonic music into the christian church?\n",
      "ACTUAL: Who first brought polyphonic music into the Christian church?\n",
      "PREDICTION: Which person or group brought polyphonic music into the christian church through\n",
      "\n",
      "\n",
      "INPUT: Where does fertilization take place in the femal reproductive system?\n",
      "ACTUAL: Where does fertilization take place generally in the female reproductive system?\n",
      "PREDICTION: Where does fertilization take place in the femal reproductive system?\n",
      "\n",
      "\n",
      "INPUT: Where does fertilization take place in the femal reproductive system?\n",
      "ACTUAL: Where does fertilization take place in the human female reproductive system?\n",
      "PREDICTION: Where does fertilization take place in the femal reproductive system?\n",
      "\n",
      "\n",
      "INPUT: When does the new thor ragnarok come out?\n",
      "ACTUAL: When does the new film thor ragnarok come out in El  Capitan Theatre?\n",
      "PREDICTION: When does the new thor ragnarok come out for pre-release in\n",
      "\n",
      "\n",
      "INPUT: When does the new thor ragnarok come out?\n",
      "ACTUAL: When does the new film thor ragnarok come out in the United States and Canada?\n",
      "PREDICTION: When does the new thor ragnarok come out for pre-release in\n",
      "\n",
      "\n",
      "INPUT: When does the new thor ragnarok come out?\n",
      "ACTUAL: When does the new film thor ragnarok come out on the Gold Coast?\n",
      "PREDICTION: When does the new thor ragnarok come out for pre-release in\n",
      "\n",
      "\n",
      "INPUT: When does the new thor ragnarok come out?\n",
      "ACTUAL: When does the new film thor ragnarok come out in all of Australia?\n",
      "PREDICTION: When does the new thor ragnarok come out for pre-release in\n",
      "\n",
      "\n",
      "INPUT: When does the new thor ragnarok come out?\n",
      "ACTUAL: When does the new film score thor ragnarok come out?\n",
      "PREDICTION: When does the new thor ragnarok come out for pre-release in\n",
      "\n",
      "\n",
      "INPUT: Who played bonnie in gone with the wind?\n",
      "ACTUAL: Who played bonnie in the 1939 film gone with the wind?\n",
      "PREDICTION: Who played Bonnie in Gone with the Wind (1961 TV series)?\n",
      "\n",
      "\n",
      "INPUT: Who played bonnie in gone with the wind?\n",
      "ACTUAL: Who played bonnie in the 2008 musical gone with the wind?\n",
      "PREDICTION: Who played Bonnie in Gone with the Wind (1961 TV series)?\n",
      "\n",
      "\n",
      "INPUT: When was the last time pennsylvania voted republican?\n",
      "ACTUAL: When was the last date pennsylvania voted republican in 2016?\n",
      "PREDICTION: When was the last time pennsylvania voted republican in presidential elections?\n",
      "\n",
      "\n",
      "INPUT: When was the last time pennsylvania voted republican?\n",
      "ACTUAL: When was the last date pennsylvania voted republican in 1988?\n",
      "PREDICTION: When was the last time pennsylvania voted republican in presidential elections?\n",
      "\n",
      "\n",
      "INPUT: When was the last time pennsylvania voted republican?\n",
      "ACTUAL: When was the last date pennsylvania voted republican in 1984?\n",
      "PREDICTION: When was the last time pennsylvania voted republican in presidential elections?\n",
      "\n",
      "\n",
      "INPUT: Who wrote what you won't do for love?\n",
      "ACTUAL: Who wrote the novel what you won't do for love?\n",
      "PREDICTION: Who wrote the song \"What You Won't Do for Love\"?\n",
      "\n",
      "\n",
      "INPUT: Who wrote what you won't do for love?\n",
      "ACTUAL: Who wrote the song what you won't do for love?\n",
      "PREDICTION: Who wrote the song \"What You Won't Do for Love\"?\n",
      "\n",
      "\n",
      "INPUT: Who won the last 20 over world cup?\n",
      "ACTUAL: What team won the 2016 Mens ICC World Twenty20?\n",
      "PREDICTION: Who won the last 20 over world cup in 2014?\n",
      "\n",
      "\n",
      "INPUT: Who won the last 20 over world cup?\n",
      "ACTUAL: What team won the 2014 Mens ICC World Twenty20?\n",
      "PREDICTION: Who won the last 20 over world cup in 2014?\n",
      "\n",
      "\n",
      "INPUT: Who won the last 20 over world cup?\n",
      "ACTUAL: What team won the 2012 Mens ICC World Twenty20?\n",
      "PREDICTION: Who won the last 20 over world cup in 2014?\n",
      "\n",
      "\n",
      "INPUT: When was the last world cup and who won?\n",
      "ACTUAL: When was the last Cricket world cup?\n",
      "PREDICTION: When was the last men's fifa world cup and who won?\n",
      "\n",
      "\n",
      "INPUT: When was the last world cup and who won?\n",
      "ACTUAL: Who won the last Cricket world cup?\n",
      "PREDICTION: When was the last men's fifa world cup and who won?\n",
      "\n",
      "\n",
      "INPUT: When was the last world cup and who won?\n",
      "ACTUAL: When was the last Rugby world cup?\n",
      "PREDICTION: When was the last men's fifa world cup and who won?\n",
      "\n",
      "\n",
      "INPUT: When was the last world cup and who won?\n",
      "ACTUAL: Who won the last Rugby world cup?\n",
      "PREDICTION: When was the last men's fifa world cup and who won?\n",
      "\n",
      "\n",
      "INPUT: When was the last world cup and who won?\n",
      "ACTUAL: When was the last FIFA world cup?\n",
      "PREDICTION: When was the last men's fifa world cup and who won?\n",
      "\n",
      "\n",
      "INPUT: When was the last world cup and who won?\n",
      "ACTUAL: Who won the last FIFA world cup?\n",
      "PREDICTION: When was the last men's fifa world cup and who won?\n",
      "\n",
      "\n",
      "INPUT: What was all found in the dead sea scrolls?\n",
      "ACTUAL: What was all found in the dead sea scrolls regarding works later included the Hebrew Bible canon?\n",
      "PREDICTION: What was all found in the dead sea scrolls in the Dead Sea Scrolls?\n",
      "\n",
      "\n",
      "INPUT: What was all found in the dead sea scrolls?\n",
      "ACTUAL: In the dead sea scrolls, they found evidence of diversity of religious through what time period?\n",
      "PREDICTION: What was all found in the dead sea scrolls in the Dead Sea Scrolls?\n",
      "\n",
      "\n",
      "INPUT: What was all found in the dead sea scrolls?\n",
      "ACTUAL: What type of scientific study benefited from what was all found in the dead sea scrolls in terms of languages?\n",
      "PREDICTION: What was all found in the dead sea scrolls in the Dead Sea Scrolls?\n",
      "\n",
      "\n",
      "INPUT: Who won the battle of iwo jima ww2?\n",
      "ACTUAL: Which country won the battle of iwo jima ww2?\n",
      "PREDICTION: Who won the battle of iwo jima www2 in terms\n",
      "\n",
      "\n",
      "INPUT: Who won the battle of iwo jima ww2?\n",
      "ACTUAL: Who commanded all the US expeditionary troops to win the battle of iwo jima ww2?\n",
      "PREDICTION: Who won the battle of iwo jima www2 in terms\n",
      "\n",
      "\n",
      "INPUT: Who won the battle of iwo jima ww2?\n",
      "ACTUAL: Who commanded the 3rd Marine Division to win the battle of iwo jima ww2?\n",
      "PREDICTION: Who won the battle of iwo jima www2 in terms\n",
      "\n",
      "\n",
      "INPUT: Who won the battle of iwo jima ww2?\n",
      "ACTUAL: Who commanded the US Fifth Fleet to win the battle of iwo jima ww2?\n",
      "PREDICTION: Who won the battle of iwo jima www2 in terms\n",
      "\n",
      "\n",
      "INPUT: Who won the battle of iwo jima ww2?\n",
      "ACTUAL: Who commanded the US V Amphibious Corps to win the battle of iwo jima ww2?\n",
      "PREDICTION: Who won the battle of iwo jima www2 in terms\n",
      "\n",
      "\n",
      "INPUT: Who won the battle of iwo jima ww2?\n",
      "ACTUAL: Who commanded the 5th Marine Division to win the battle of iwo jima ww2?\n",
      "PREDICTION: Who won the battle of iwo jima www2 in terms\n",
      "\n",
      "\n",
      "INPUT: Who won the battle of iwo jima ww2?\n",
      "ACTUAL: Who commanded the 4th Marine Division to win the battle of iwo jima ww2?\n",
      "PREDICTION: Who won the battle of iwo jima www2 in terms\n",
      "\n",
      "\n",
      "INPUT: Where is the mouth of the river severn and what is it called?\n",
      "ACTUAL: Where is the mouth of the River Severn in England?\n",
      "PREDICTION: Where is the mouth of the river severn and what is it called?\n",
      "\n",
      "\n",
      "INPUT: Where is the mouth of the river severn and what is it called?\n",
      "ACTUAL: What is the mouth of the River Severn called?\n",
      "PREDICTION: Where is the mouth of the river severn and what is it called?\n",
      "\n",
      "\n",
      "INPUT: Where is the mouth of the river severn and what is it called?\n",
      "ACTUAL: Where is the mouth of the Severn River in United States?\n",
      "PREDICTION: Where is the mouth of the river severn and what is it called?\n",
      "\n",
      "\n",
      "INPUT: Where is the mouth of the river severn and what is it called?\n",
      "ACTUAL: What is the mouth of the Severn River in United States called?\n",
      "PREDICTION: Where is the mouth of the river severn and what is it called?\n",
      "\n",
      "\n",
      "INPUT: Who was the captain of the cornelia marie?\n",
      "ACTUAL: Who was the captain of the cornelia marie until 2010?\n",
      "PREDICTION: Who was the captain of the cornelia marie from 1862 to 18\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Generate the outputs using the model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Decode predictions\u001b[39;00m\n\u001b[1;32m     13\u001b[0m pred_texts \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1752\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1746\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1747\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1748\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1766\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3091\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3089\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3091\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3099\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1577\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1573\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1574\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1575\u001b[0m         )\n\u001b[0;32m-> 1577\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1595\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1596\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1463\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1457\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1458\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1459\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1460\u001b[0m     )\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1463\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1316\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1304\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1305\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         use_cache,\n\u001b[1;32m   1314\u001b[0m     )\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1316\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:655\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m    654\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    664\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:285\u001b[0m, in \u001b[0;36mBartAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# partitioned across GPUs when using tensor-parallelism.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m--> 285\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights_reshaped, past_key_value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()  # make sure the model is in evaluation mode\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader_test:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Generate the outputs using the model\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Decode predictions\n",
    "        pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        # print(pred_texts)\n",
    "        predictions.extend(pred_texts)\n",
    "        \n",
    "        # Decode the reference labels\n",
    "        ref_texts = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "        input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        \n",
    "        for i in range(len(pred_texts)):\n",
    "            print(f\"INPUT: {input_texts[i]}\")\n",
    "            print(f\"ACTUAL: {ref_texts[i]}\")\n",
    "            print(f\"PREDICTION: {pred_texts[i]}\")\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        references.extend([[ref] for ref in ref_texts])\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e7038-a649-4369-945e-e280584b4e82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
