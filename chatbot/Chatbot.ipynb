{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dd82db-5eb2-47bf-ad38-0507692c5255",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cddf8bf-a68e-47f4-965d-105a057b0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91371c3e-f091-439b-bc5c-1be2307548c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "sys.path.append('../..')\n",
    "import python_utils\n",
    "import torch\n",
    "import ast\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BertTokenizer, BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# from dotenv import loader_dotenv, find_dotenv\n",
    "\n",
    "API_KEY = ''\n",
    "client = OpenAI(api_key = API_KEY)\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c61168-fb2f-48d9-8b03-27c8fa132d0a",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0e592a-e0e6-472b-8c62-6fd0ab44f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function receives an array of 'messages' and returns an output based on this array.\n",
    "# INPUTS:\n",
    "#      * messages: an array of messages between user and gpt model.\n",
    "#      * TODO: complete\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo-16k\", temperature=0, max_tokens=500):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    last_message = response.choices[0].message\n",
    "    return last_message.content\n",
    "\n",
    "def get_prompt_ambig_category(user_prompt):\n",
    "    BERT_tokenizer = BertTokenizer.from_pretrained('../models/final_pretrained_saves_BERT')\n",
    "    BERT_model = BertForSequenceClassification.from_pretrained('../models/final_pretrained_saves_BERT')\n",
    "    BERT_model.eval()\n",
    "    \n",
    "    inputs = BERT_tokenizer(user_prompt, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = BERT_model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    query_classification = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    return query_classification\n",
    "\n",
    "\n",
    "def get_prompt_rephrase(ambiguous_prompt):\n",
    "    BART_tokenizer = BartTokenizer.from_pretrained('../models/final_pretrained_saves')\n",
    "    BART_model = BartForConditionalGeneration.from_pretrained('../models/final_pretrained_saves')    \n",
    "    BART_model.eval()\n",
    "    \n",
    "    inputs = BART_tokenizer.encode(ambiguous_prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = BART_model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    disambiguated_question = BART_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return disambiguated_question\n",
    "\n",
    "\n",
    "def get_tokenized_responses(response):\n",
    "    tokenized_response = response.split()\n",
    "    return tokenized_response\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def embed_user_prompt(user_prompt, model=\"text-embedding-ada-002\"):\n",
    "    \n",
    "    # try:\n",
    "    #     # Create the embedding\n",
    "    #     response = client.embeddings.create(input=[user_prompt], model=model)\n",
    "    #     embedding = response.data[0].embedding\n",
    "    #     return embedding\n",
    "    # except Exception as e:\n",
    "    #     # Handle exceptions\n",
    "    #     print(f\"An error occurred: {e}\")\n",
    "    #     return None\n",
    "    \n",
    "    response = client.embeddings.create(input=[user_prompt], model=model)\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding\n",
    "    \n",
    "\n",
    "# TODO\n",
    "def get_content_from_database(embedded_user_prompt, df):\n",
    "    \n",
    "    embedded_user_prompt = embedded_user_prompt.reshape(1, -1)\n",
    "    page_embeddings = np.vstack(df['page_embedding'])\n",
    "    similarities = cosine_similarity(embedded_user_prompt, page_embeddings)\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "\n",
    "    return df.iloc[most_similar_index]['page_content']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136ca99-fca6-47e0-a473-2528506f6df4",
   "metadata": {},
   "source": [
    "## Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "149c8da9-7aef-4b02-b63a-ce27d9e0b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../../data/GPT/GPT_test_final_csv')\n",
    "test_df = test_df.drop(columns=['question_type', 'viewed_doc_titles', 'disambiguated_question'])\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820d6ca9-b710-4d56-9862-0cb7ceb40e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(test_df['page_content'])\n",
    "test_df = test_df.drop(columns=['page_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb1e69b-ceb6-4839-bf24-5013a1c864c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>~~In chemistry, a mixture is a material made u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        page_content\n",
       "0  ~~The Constitution of the State of Texas is th...\n",
       "1  ~~The Constitution of the State of Texas is th...\n",
       "2  ~~The Man Who Shot Liberty Valance is a 1962 A...\n",
       "3  ~~The Man Who Shot Liberty Valance is a 1962 A...\n",
       "4  ~~In chemistry, a mixture is a material made u..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CITATION: https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "embeddings_df['page_content'] = embeddings_df['page_content'].apply(remove_html_tags)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77bdae49-f108-4f87-abea-d62d6e0ee55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = \"text-embedding-ada-002\"\n",
    "# embeddings = []\n",
    "\n",
    "# # CITATION: https://platform.openai.com/docs/guides/embeddings/use-cases\n",
    "# \n",
    "# def get_embedding(text, model=\"text-embedding-ada-002\", max_length = 8000):\n",
    "#     # text = text.replace(\"\\n\", \" \")[:max_length]\n",
    "#     # return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "#     text = text.replace(\"\\n\", \" \")[:max_length]\n",
    "#     try:\n",
    "#         return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error occurred: {e}\")\n",
    "#         time.sleep(10)  # Wait for 10 seconds before retrying\n",
    "#         return None\n",
    "    \n",
    "# embeddings = []\n",
    "# counter = 0\n",
    "# for row in embeddings_df['page_content']:\n",
    "#     if counter%200==0:\n",
    "#         print(counter)\n",
    "        \n",
    "#     embedding = get_embedding(row, model='text-embedding-ada-002')\n",
    "#     embeddings.append(embedding)\n",
    "#     counter+=1\n",
    "\n",
    "# embeddings_df['page_embedding'] = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3cc67a5-9199-46ef-99b0-325328684005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_df.to_csv('embedded_page_contents.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d96550-60b2-4f57-a87c-b17d17b04a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_csv('embedded_page_contents.csv')\n",
    "embeddings_df['page_embedding'] = embeddings_df['page_embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aadaa6e-2f5e-490d-a12d-aedc5efb6ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_content</th>\n",
       "      <th>page_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "      <td>[-0.0027238428592681885, 0.020244302228093147,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "      <td>[-0.0027419731486588717, 0.020274952054023743,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "      <td>[-0.020264700055122375, -0.018238229677081108,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "      <td>[-0.020236164331436157, -0.0182228721678257, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>~~In chemistry, a mixture is a material made u...</td>\n",
       "      <td>[0.006561756134033203, 0.02816181816160679, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        page_content  \\\n",
       "0  ~~The Constitution of the State of Texas is th...   \n",
       "1  ~~The Constitution of the State of Texas is th...   \n",
       "2  ~~The Man Who Shot Liberty Valance is a 1962 A...   \n",
       "3  ~~The Man Who Shot Liberty Valance is a 1962 A...   \n",
       "4  ~~In chemistry, a mixture is a material made u...   \n",
       "\n",
       "                                      page_embedding  \n",
       "0  [-0.0027238428592681885, 0.020244302228093147,...  \n",
       "1  [-0.0027419731486588717, 0.020274952054023743,...  \n",
       "2  [-0.020264700055122375, -0.018238229677081108,...  \n",
       "3  [-0.020236164331436157, -0.0182228721678257, 0...  \n",
       "4  [0.006561756134033203, 0.02816181816160679, -0...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03be04ea-6f22-4b09-a1c1-46d41a97681b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ambiguous_question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>What is the classification of a loggerhead sea...</td>\n",
       "      <td>Testudines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>Where will the super rugby final be played?</td>\n",
       "      <td>Ellis Park Stadium, Johannesburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>Who controls the house of representatives righ...</td>\n",
       "      <td>The Democratic Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>Who is opening for luke bryan kill the lights ...</td>\n",
       "      <td>Brett Eldredge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Who hit the longest drive in golf history?</td>\n",
       "      <td>Mike Dobbyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ambiguous_question  \\\n",
       "596   What is the classification of a loggerhead sea...   \n",
       "4507        Where will the super rugby final be played?   \n",
       "3049  Who controls the house of representatives righ...   \n",
       "2957  Who is opening for luke bryan kill the lights ...   \n",
       "8            Who hit the longest drive in golf history?   \n",
       "\n",
       "                                answer  \n",
       "596                         Testudines  \n",
       "4507  Ellis Park Stadium, Johannesburg  \n",
       "3049              The Democratic Party  \n",
       "2957                    Brett Eldredge  \n",
       "8                          Mike Dobbyn  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_SAMPLE = test_df.sample(n=100, random_state=42)\n",
    "test_df_SAMPLE.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919fba1-5275-47b2-8a46-da682d689f76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot with Ambiguous Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba39e4b2-587f-4176-9f01-59b374065bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# panels = []\n",
    "# delimiter = \"~~~~\"\n",
    "# start_convo_flag = True\n",
    "\n",
    "# context = [{'role': 'system', 'content':f\"\"\"\n",
    "\n",
    "# You are a chatbot that for 'fact-based' or 'direct' questioning. You will be provided a question and you must provide an answer. \\\n",
    "# You answer must be short. Do not include explanations to the provided question. Here is an example:\n",
    "\n",
    "# Question: What is the capital of Japan?\n",
    "\n",
    "# Answer: Tokyo.\n",
    "\n",
    "\n",
    "# \"\"\"}] # accumulate messages\n",
    "\n",
    "\n",
    "# context.append({'role':'user', 'content':f\"{delimiter}Hi!{delimiter}\"})\n",
    "# init_response = get_completion_from_messages(context)\n",
    "\n",
    "# print(\"-- Chatbot: \", init_response, \"\\n\")\n",
    "# while start_convo_flag:\n",
    "#     print(\"-- User: \")\n",
    "#     user_input = input()\n",
    "#     print()\n",
    "#     if user_input == \"stop\":\n",
    "#         break\n",
    "        \n",
    "#     # Check if conversation is an ambiguous question or not\n",
    "#     is_ambiguous = get_prompt_ambig_category(user_input)\n",
    "#     print(\"IS AMBIGUOUS? \", is_ambiguous)\n",
    "    \n",
    "#     # conversation_history = [entry for entry in context if entry['role'] != 'system']\n",
    "#     # continuation_flag = int(is_continuation(conversation_history, user_input)\n",
    "    \n",
    "#     # Match question with NQ Dataset\n",
    "#     db_content = get_content_from_database(user_input)\n",
    "    \n",
    "#     if is_ambiguous == 1:\n",
    "#         print(\"QUESTION IS AMBIGUOUS!\")\n",
    "#         rephrased_user_input = get_prompt_rephrase(user_input)\n",
    "#         print(\"REPHRASED QUESTION: \", rephrased_user_input)\n",
    "#         user_input = rephrased_user_input\n",
    "            \n",
    "#     # Add System instructions\n",
    "#     context.append({'role':'system', 'content': f\"\"\"Use the provided content to answer the question.\\\n",
    "#                     Do not make assumptions or use information from your pre-trained data. Use this information: {db_content}\"\"\"})\n",
    "\n",
    "#     # Input User's question\n",
    "#     context.append({'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"})\n",
    "#     response = get_completion_from_messages(context)\n",
    "#     print(\"-- Chatbot: \", response, \"\\n\")\n",
    "#     context.append({'role': 'assistant', 'content': f\"{response}\"})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d928b9f-d02c-46c2-a89b-d8b3757aea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- User prompt:  What is the classification of a loggerhead sea turtle?\n",
      "QUESTION IS AMBIGUOUS!\n",
      " -- New User prompt:  What is the general classification of a loggerhead sea turtle?\n",
      "Contents:  ~~The subspecific classification of the loggerhead sea turtle is debated, but most \n",
      "authors consider it a single polymorphic species.~~Sea turtles (superfamily Chelonioidea), sometimes called marine turtles, are \n",
      "reptiles of the order Testudines and of the suborder Cryptodira. The seven \n",
      "existing species of sea turtles are the green sea turtle, loggerhead ... An \n",
      "additional three species are classified as &quot;Vulnerable&quot;. The flatback sea turtle is \n",
      "considered as &quot;Data&nbsp;...~~Cheloniidae is a family of typically large marine turtles that are characterised by \n",
      "their common ... The green and loggerhead sea turtles are categorized as \n",
      "endangered, Olive Ridleys are classified as vulnerable, Kemp&#39;s Ridleys, and \n",
      "Hawksbills sea turtles are critically endangered and the Flat Back sea turtle does \n",
      "not have&nbsp;...~~The green sea turtle is a member of the tribe Chelonini. A 1993 study clarified the \n",
      "status of genus Chelonia with respect to the other marine turtles. The carnivorous \n",
      "Eretmochelys (hawksbill), Caretta (loggerhead) and&nbsp;...~~The hawksbill sea turtle (Eretmochelys imbricata) is a Critically Endangered sea \n",
      "turtle belonging to the family Cheloniidae. It is the only extant species in the \n",
      "genus Eretmochelys. ... then-species being the second species in the genus. \n",
      "Caretta is the genus of the hawksbill&#39;s much larger relative, the loggerhead sea \n",
      "turtle.~~The alligator snapping turtle (Macrochelys temminckii) is a species of turtle in the \n",
      "family ... 1 Taxonomy; 2 Common name; 3 Distribution and habitat; 4 Description \n",
      "... &quot;the loggerhead snapper&quot; (not to be confused with the loggerhead sea turtle&nbsp;...~~Ridley sea turtles (Lepidochelys) are a genus of sea turtle comprising two \n",
      "species: Kemp&#39;s ridley sea turtle and the olive ridley sea turtle. Kemp&#39;s ridley sea\n",
      "&nbsp;...~~Pacifichelys is an extinct genus of sea turtle from the Middle Miocene of Peru (\n",
      "Pisco Formation) ... Like the living Ridley and loggerhead sea turtles, Pacifichelys \n",
      "was durophagous, consuming hard-shelled organisms with crushing jaws.~~There is only one land reptile species native to Ireland, the viviparous or common \n",
      "lizard. It appears to have a widespread distribution across the entire island&nbsp;...~~The olive ridley sea turtle (Lepidochelys olivacea), also known commonly as the \n",
      "Pacific ridley sea turtle, is a species of turtle in the family Cheloniidae.~~\n",
      "['sea', 'turtle'] ['Testudines']\n",
      "BLEU Score:  0\n",
      "METEOR Score:  0.0\n",
      " -- User prompt:  Where will the super rugby final be played?\n",
      "QUESTION IS AMBIGUOUS!\n",
      " -- New User prompt:  Where was the super rugby final played in 2017?\n",
      "Contents:  ~~The 2019 Super Rugby season was the 24th season of Super Rugby, an annual \n",
      "rugby union ... Standings[edit]. The final standings for the 2019 Super Rugby \n",
      "season are: ... First match(es) will be played on 15 February 2019. Source:&nbsp;...~~Super Rugby is a professional men&#39;s rugby union international club competition \n",
      "involving ... The two winners then played the final at the home ground of the top \n",
      "surviving seed. ... SANZAR announced in 2009 the addition of a fifth Australian \n",
      "team that would play in the expanded &quot;Super Rugby&quot; competition in 2011.~~The 2011 Super Rugby Final, was played between the Queensland Reds from \n",
      "Australia and ... With the score locked at 13–13, Reds scrum-half Will Genia \n",
      "broke the deadlock with a solo 30 metre run for the standout try of the match. The \n",
      "Reds&nbsp;...~~The 2018 Super Rugby season was the 23rd season of Super Rugby, an annual \n",
      "rugby union ... In the group stages, there are 19 rounds of matches, with each \n",
      "team playing 16 ... as do the next five teams with the best records across the \n",
      "three conferences, known ... The final standings for the 2018 Super Rugby \n",
      "season were:&nbsp;...~~The 2012 Super Rugby final was played between the South African Sharks and \n",
      "the New ... The top six teams after the regular season would advance to the finals\n",
      ". Unlike previous years, there was a three-week break between Week 15 and&nbsp;...~~The 2016 Super Rugby Final was played between the Hurricanes and the Lions. \n",
      "It was the 21st final in the Super Rugby competition&#39;s history and the first under&nbsp;...~~The 2014 Super Rugby Final, was played between the New South Wales \n",
      "Waratahs from Australia and the Crusaders from New Zealand on 2 August 2014.~~Super Rugby is the major professional rugby union competition in the Southern \n",
      "Hemisphere. ... South African conference 1 teams will play Australian and New \n",
      "Zealand teams in ... The highlighted team won that season&#39;s Super Rugby final.~~The 2020 Super Rugby season is the 25th season of Super Rugby, an \n",
      "international men&#39;s ... On 14 March 2020, it was announced that play will be \n",
      "suspended after the conclusion ... 15 Teams will partake in the 2020 edition of \n",
      "Super Rugby. ... The winner of each conference qualifies for the Super Rugby \n",
      "finals, along with the&nbsp;...~~The 2011 Super Rugby season was the first season of the new 15-team format \n",
      "for the Super ... During this season, the first ever Super Rugby game was played \n",
      "outside the SANZAR region, ... The final was played at Suncorp Stadium in \n",
      "Brisbane between the ... &quot;Team with &quot;Most Wins&quot; will finish higher in Super Rugby\n",
      "&quot;.~~\n",
      "['unknown'] ['Ellis', 'Park', 'Stadium', 'Johannesburg']\n",
      "BLEU Score:  0\n",
      "METEOR Score:  0.0\n",
      " -- User prompt:  Who controls the house of representatives right now?\n",
      "QUESTION IS AMBIGUOUS!\n",
      " -- New User prompt:  Who controls the house of representatives in the US House of Representatives in 2017?\n",
      "Contents:  ~~There was also a shift from the 1990s to greater control of the legislative program \n",
      "by the majority party; the power of party leaders (especially the speaker) grew&nbsp;...~~The 2018 United States House of Representatives elections were held on \n",
      "November 6, 2018, ... The Democrats also won the popular vote by a margin of \n",
      "8.6%, the largest margin on ... The Democratic Party won control of the House of \n",
      "Representatives in the 2018 ... &quot;What happens now that Democrats will retake the \n",
      "House&quot;.~~The 116th United States Congress is the current meeting of the legislative branch \n",
      "of the United States federal government, composed of the Senate and the House \n",
      "of Representatives. It convened in Washington, D.C., on January 3, 2019, and \n",
      "will end on ... the Democratic Party won a new majority in the House, while the \n",
      "Republican&nbsp;...~~If no candidate wins a majority of the &quot;votes cast for a person by name,&quot; then the \n",
      "roll call is repeated until a speaker is elected. Multiple roll calls have been&nbsp;...~~The speaker of the United States House of Representatives is the presiding \n",
      "officer of the United ... Since 1839, the House has elected speakers by roll call \n",
      "vote. ... To be elected speaker, a candidate must receive an absolute majority of \n",
      "the votes ... who had been elected as a Know Nothing but was now largely \n",
      "identified with&nbsp;...~~The Texas House of Representatives is the lower house of the bicameral Texas \n",
      "Legislature. It consists of 150 members who are elected from single-member&nbsp;...~~The Speaker is elected by the majority party caucus followed by confirmation of \n",
      "the full House through the passage of a House Resolution. As well as presiding&nbsp;...~~The One Hundred Fourteenth United States Congress was a meeting of the \n",
      "legislative branch of the United States of America federal government, composed \n",
      "of the United States Senate and the United States House of Representatives. It \n",
      "met in Washington, D.C. from January 3, 2015, to January 3, 2017, during ... The \n",
      "2014 elections gave the Republicans control of the Senate (and control&nbsp;...~~The Michigan House of Representatives is the lower house of the Michigan \n",
      "Legislature. ... The Clerk of the House of Representatives is elected by Members \n",
      "of the ... House and the minority leader, and the majority and minority floor \n",
      "leaders.~~Others have argued that the now-abandoned system provided for greater &quot;\n",
      "stability&quot; in the lower house. The Democratic Party won a majority of House seats \n",
      "in&nbsp;...~~\n",
      "['the', 'republican', 'party'] ['The', 'Democratic', 'Party']\n",
      "BLEU Score:  0\n",
      "METEOR Score:  0.3333333333333333\n",
      " -- User prompt:  Who is opening for luke bryan kill the lights tour?\n",
      "QUESTION IS AMBIGUOUS!\n",
      " -- New User prompt:  Who is opening for luke bryan kill the lights tour in 2015?\n",
      "Contents:  ~~The Kill the Lights Tour was the fourth headlining concert tour by American \n",
      "country music artist Luke Bryan. The tour is in support of his fifth studio album Kill \n",
      "the&nbsp;...~~Kill the Lights is the fifth studio album by American country music artist Luke \n",
      "Bryan. ... (2015), Farm Tour. ... Brian Mansfield rates the album three stars out of \n",
      "four at USA Today proffering: &quot;The hits are fine, but that&#39;s the guy who&#39;s really \n",
      "worth&nbsp;...~~Thomas Luther &quot;Luke&quot; Bryan (born July 17, 1976) is an American country music \n",
      "singer and ... During the kickoff show for his 2014 That&#39;s My Kind of Night Tour in \n",
      "Columbus, ... All six of the singles released from Bryan&#39;s Kill the Lights album \n",
      "reached ... &quot;Luke Bryan Opens Up About the Two Tragedies That Nearly Broke \n",
      "Him&nbsp;...~~Kill the Lights Tour (2016–17); Huntin&#39;, Fishin&#39; and Lovin&#39; Every Day Tour (2017); \n",
      "What Makes You Country Tour (2018). The Huntin&#39;, Fishin&#39; and Lovin&#39; Every Day \n",
      "Tour was the fifth headlining concert tour by American country music artist Luke \n",
      "Bryan. ... Bryan opens the show with &quot;Move&quot; and past hits, &quot;That&#39;s My Kind of \n",
      "Night&quot;&nbsp;...~~Kick the Dust Up Tour was the third headlining concert tour by American country \n",
      "music singer Luke Bryan, in support of his albums Crash My Party, Spring Break..\n",
      ".Checkin&#39; Out, and Kill the Lights. ... with Bryan being risen up from a platform \n",
      "under a circular stage while singing the opening number, &quot;Kick the Dust Up&quot;.~~Kill the Lights (2015), Farm Tour... Here&#39;s to the Farmer (2016), What Makes You \n",
      "Country (2017). Farm Tour... Here&#39;s to the Farmer is the eighth extended play (EP\n",
      ") by American country music singer Luke Bryan.~~&quot;Strip It Down&quot; is a song co-written and recorded by American country music artist \n",
      "Luke Bryan. It was first released to digital retailers as the first promotional single \n",
      "from his fifth studio album, Kill the Lights (2015), on July 17, 2015 and was \n",
      "released to radio on August 4 ... Dirt Road Diaries Tour (2013); That&#39;s My Kind of \n",
      "Night Tour (2014–15)&nbsp;...~~&quot;Huntin&#39;, Fishin&#39; and Lovin&#39; Every Day&quot; is a song co-written and recorded by \n",
      "American country music artist Luke Bryan for his fifth studio album, Kill the Lights\n",
      "&nbsp;...~~&quot;Kick the Dust Up&quot; is a song written by Ashley Gorley, Dallas Davidson, and Chris \n",
      "DeStefano and recorded by American country music artist Luke Bryan. It was \n",
      "released in May 2015 as the first single from Bryan&#39;s 2015 album, Kill the Lights. \n",
      "... during the last leg of his That&#39;s My Kind of Night Tour and Kick the Dust Up tour.~~It was released to American country radio on July 25, 2016 as the fifth official \n",
      "single from his 2015 album Kill the Lights. Bryan wrote this song with Michael&nbsp;...~~\n",
      "['unknown'] ['Brett', 'Eldredge']\n",
      "BLEU Score:  0\n",
      "METEOR Score:  0.0\n",
      " -- User prompt:  Who hit the longest drive in golf history?\n",
      "QUESTION IS AMBIGUOUS!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_ambiguous \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQUESTION IS AMBIGUOUS!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     rephrased_user_input \u001b[38;5;241m=\u001b[39m \u001b[43mget_prompt_rephrase\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# print(\"REPHRASED QUESTION: \", rephrased_user_input)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m rephrased_user_input\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mget_prompt_rephrase\u001b[0;34m(ambiguous_prompt)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_prompt_rephrase\u001b[39m(ambiguous_prompt):\n\u001b[1;32m     35\u001b[0m     BART_tokenizer \u001b[38;5;241m=\u001b[39m BartTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/final_pretrained_saves\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     BART_model \u001b[38;5;241m=\u001b[39m \u001b[43mBartForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../models/final_pretrained_saves\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     37\u001b[0m     BART_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     39\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m BART_tokenizer\u001b[38;5;241m.\u001b[39mencode(ambiguous_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3236\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map)\n\u001b[1;32m   3235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 3236\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3238\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1503\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: BartConfig):\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mBartModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1504\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_logits_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mshared\u001b[38;5;241m.\u001b[39mnum_embeddings)))\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39md_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mshared\u001b[38;5;241m.\u001b[39mnum_embeddings, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1373\u001b[0m, in \u001b[0;36mBartModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1370\u001b[0m padding_idx, vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpad_token_id, config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, config\u001b[38;5;241m.\u001b[39md_model, padding_idx)\n\u001b[0;32m-> 1373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mBartEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m BartDecoder(config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared)\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:938\u001b[0m, in \u001b[0;36mBartEncoder.__init__\u001b[0;34m(self, config, embed_tokens)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_source_positions \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmax_position_embeddings\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_scale \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(embed_dim) \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mscale_embedding \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m--> 938\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embed_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m embed_tokens\u001b[38;5;241m.\u001b[39mweight\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:144\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs),\n\u001b[1;32m    143\u001b[0m                             requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze)\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_weight\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:153\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fill_padding_idx_with_zero()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[38;5;241m=\u001b[39mtensor, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd)\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "panels = []\n",
    "bleu_scores_array = []\n",
    "meteor_scores_array = []\n",
    "bert_scores = []\n",
    "counter = 0\n",
    "\n",
    "delimiter = \"===\"\n",
    "\n",
    "context = [{'role': 'system', 'content':f\"\"\"\n",
    "\n",
    "You are a chatbot that for 'fact-based' or 'direct' questioning. You will be provided a question and you must provide an answer. \\\n",
    "You answer must be short. Do not include explanations to the provided question. Here is an example:\n",
    "\n",
    "Question: What is the capital of Japan?\n",
    "\n",
    "Answer: Tokyo.\n",
    "\n",
    "Answer the user's prompt, delimited by {delimiter}\n",
    "\n",
    "\"\"\"}] # accumulate messages\n",
    "\n",
    "\n",
    "for index, row in test_df_SAMPLE.iterrows(): # 200 samples!\n",
    "    counter+=1\n",
    "    if counter%10==0:\n",
    "        print(counter)\n",
    "        # time.sleep(3)\n",
    "    user_input = row['ambiguous_question']\n",
    "    print(\" -- User prompt: \", user_input)\n",
    "\n",
    "    # Check if conversation is an ambiguous question or not\n",
    "    is_ambiguous = get_prompt_ambig_category(user_input)\n",
    "    # print(\"IS AMBIGUOUS? \", is_ambiguous)\n",
    "    \n",
    "\n",
    "    if is_ambiguous == 1:\n",
    "        print(\"QUESTION IS AMBIGUOUS!\")\n",
    "        rephrased_user_input = get_prompt_rephrase(user_input)\n",
    "        # print(\"REPHRASED QUESTION: \", rephrased_user_input)\n",
    "        user_input = rephrased_user_input\n",
    "        print(\" -- New User prompt: \", user_input)\n",
    "\n",
    "    embedded_user_prompt =np.array(embed_user_prompt(user_input))\n",
    "\n",
    "    db_page_contents = get_content_from_database(embedded_user_prompt, embeddings_df)\n",
    "    print(\"Contents: \", db_page_contents)\n",
    "\n",
    "    # Add System instructions\n",
    "    context.append({'role':'system', 'content': f\"\"\"Use the provided content to answer the question.\\\n",
    "                    Do not make assumptions or use information from your pre-trained data. If the answer is not explicitly mentioned in the provided information, \\\n",
    "                    reply with 'unknown'. Use this information: {db_page_contents}\"\"\"})\n",
    "\n",
    "    # Input User's question\n",
    "    context.append({'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"})\n",
    "    response = get_completion_from_messages(context).lower()\n",
    "\n",
    "    # BLEU SCORE\n",
    "    response = remove_punctuation(response)\n",
    "    tokenized_response = get_tokenized_responses(response)\n",
    "    tokenized_actual = get_tokenized_responses(remove_punctuation(row['answer']))\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    # CITATION: Had to change this line of code to work with small answers. Help from ChatGPT.\n",
    "    bleu_score = sentence_bleu([tokenized_response], tokenized_actual, weights=(0.5, 0.5), smoothing_function=smooth_fn)\n",
    "    print(tokenized_response,tokenized_actual)\n",
    "    print(\"BLEU Score: \", bleu_score)\n",
    "    bleu_scores_array.append(bleu_score)\n",
    "\n",
    "    # METEOR SCORE\n",
    "    meteor_scored = meteor_score([tokenized_response], tokenized_actual)\n",
    "    print(\"METEOR Score: \", meteor_scored)\n",
    "    meteor_scores_array.append(meteor_scored)\n",
    "    \n",
    "    context = context[:1] # reset the context array\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "# print(bleu_scores_array, meteor_scores_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4c0775-d8a2-48c8-8fa1-ebea192e2910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bill of rights\n",
      "['the', 'bill', 'of', 'rights']\n",
      "\n",
      "bill of rights\n",
      "['bill', 'of', 'rights']\n",
      "0.7165313105737893\n"
     ]
    }
   ],
   "source": [
    "# response = remove_punctuation(response)\n",
    "\n",
    "# tokenized_response = get_tokenized_responses(response)\n",
    "# tokenized_actual = get_tokenized_responses(test_df.iloc[0]['answer'])\n",
    "# print(response)\n",
    "# print(tokenized_response)\n",
    "# print()\n",
    "# print(test_df.iloc[0]['answer'])\n",
    "# print(tokenized_actual)\n",
    "\n",
    "# smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "# # CITATION: Had to change this line of code to work with small answers. Help from ChatGPT.\n",
    "# bleu_score = sentence_bleu([tokenized_response], tokenized_actual, weights=(0.5, 0.5), smoothing_function=smooth_fn)\n",
    "\n",
    "# print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "865cab0b-b3fd-4a1f-9798-1e78295edae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11366781666441263\n"
     ]
    }
   ],
   "source": [
    "# print([tokenized_response])\n",
    "# print(tokenized_actual)\n",
    "\n",
    "# meteor_scores = meteor_score([tokenized_response], tokenized_actual)\n",
    "# print(meteor_scores)\n",
    "\n",
    "average_meteor_score = sum(meteor_scores_array) / len(meteor_scores_array)\n",
    "print(average_meteor_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36fb368f-4002-4728-a6cd-d7bfab3e517e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016821592920681017\n"
     ]
    }
   ],
   "source": [
    "average_bleu_score = sum(bleu_scores_array) / len(bleu_scores_array)\n",
    "print(average_bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a31bf7-d3f8-4a31-b133-7068b1166454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - Change script to test on the test AmbigNQ data\n",
    "# - Prompt-engineer GPT model to output short answers.\n",
    "# - Extract each ChatGPT output and Compare answers using embedding model and thus euclidean similarity?\n",
    "\n",
    "# Potential Improvements\n",
    "# - Hallucination tracking\n",
    "# - RAG system with GoogleNQ dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988538a-b781-4fea-831d-c46630bba684",
   "metadata": {},
   "source": [
    "## Chatbot without Ambiguous Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1edc1a37-741b-4545-8c8b-f93e879e4a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "panels = []\n",
    "bleu_scores_array = []\n",
    "meteor_scores_array = []\n",
    "bert_scores = []\n",
    "counter = 0\n",
    "\n",
    "delimiter = \"===\"\n",
    "\n",
    "context = [{'role': 'system', 'content':f\"\"\"\n",
    "\n",
    "You are a chatbot that for 'fact-based' or 'direct' questioning. You will be provided a question and you must provide an answer. \\\n",
    "You answer must be short. Do not include explanations to the provided question. Here is an example:\n",
    "\n",
    "Question: What is the capital of Japan?\n",
    "\n",
    "Answer: Tokyo.\n",
    "\n",
    "Answer the user's prompt, delimited by {delimiter}\n",
    "\n",
    "\"\"\"}] # accumulate messages\n",
    "\n",
    "\n",
    "for index, row in test_df_SAMPLE.iterrows(): # 100 samples!\n",
    "    counter+=1\n",
    "    if counter%10==0:\n",
    "        print(counter)\n",
    "        # time.sleep(3)\n",
    "    user_input = row['ambiguous_question']\n",
    "    # print(\" -- User prompt: \", user_input)\n",
    "\n",
    "\n",
    "    embedded_user_prompt =np.array(embed_user_prompt(user_input))\n",
    "\n",
    "    db_page_contents = get_content_from_database(embedded_user_prompt, embeddings_df)\n",
    "    # print(\"Contents: \", db_page_contents)\n",
    "\n",
    "    # Add System instructions\n",
    "    context.append({'role':'system', 'content': f\"\"\"Use the provided content to answer the question.\\\n",
    "                    Do not make assumptions or use information from your pre-trained data. If the answer is not explicitly mentioned in the provided information, \\\n",
    "                    reply with 'unknown'. Use this information: {db_page_contents}\"\"\"})\n",
    "\n",
    "    # Input User's question\n",
    "    context.append({'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"})\n",
    "    response = get_completion_from_messages(context).lower()\n",
    "\n",
    "    # BLEU SCORE\n",
    "    response = remove_punctuation(response)\n",
    "    tokenized_response = get_tokenized_responses(response)\n",
    "    tokenized_actual = get_tokenized_responses(remove_punctuation(row['answer']))\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    # CITATION: Had to change this line of code to work with small answers. Help from ChatGPT.\n",
    "    bleu_score = sentence_bleu([tokenized_response], tokenized_actual, weights=(0.5, 0.5), smoothing_function=smooth_fn)\n",
    "    # print(tokenized_response,tokenized_actual)\n",
    "    # print(\"BLEU Score: \", bleu_score)\n",
    "    bleu_scores_array.append(bleu_score)\n",
    "\n",
    "    # METEOR SCORE\n",
    "    meteor_scored = meteor_score([tokenized_response], tokenized_actual)\n",
    "    # print(\"METEOR Score: \", meteor_scored)\n",
    "    meteor_scores_array.append(meteor_scored)\n",
    "    \n",
    "    context = context[:1] # reset the context array\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "# print(bleu_scores_array, meteor_scores_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f68bae6-ad31-4a52-ba8a-443b0d5b19ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16436857054788823\n",
      "0.02288390667192786\n"
     ]
    }
   ],
   "source": [
    "# Use the provided content to answer the question.\\\n",
    "#                     Do not make assumptions or use information from your pre-trained data. If the answer is not explicitly mentioned in the provided information, \\\n",
    "#                     use only the provided information to with your best guess without writing a full sentence. Remember, your answer must be short. Use this information: {db_page_contents}\n",
    "\n",
    "average_meteor_score = sum(meteor_scores_array) / len(meteor_scores_array)\n",
    "print(average_meteor_score)\n",
    "average_bleu_score = sum(bleu_scores_array) / len(bleu_scores_array)\n",
    "print(average_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5292ea5-8dbc-45ad-88a4-311f8496759f",
   "metadata": {},
   "source": [
    "## Chatbot without Ambiguous Processing & No RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3e23ea1-5602-48a8-92b7-daf2c8d532e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "panels = []\n",
    "bleu_scores_array = []\n",
    "meteor_scores_array = []\n",
    "bert_scores = []\n",
    "counter = 0\n",
    "\n",
    "delimiter = \"===\"\n",
    "\n",
    "context = [{'role': 'system', 'content':f\"\"\"\n",
    "\n",
    "You are a chatbot that for 'fact-based' or 'direct' questioning. You will be provided a question and you must provide an answer. \\\n",
    "You answer must be short. Do not include explanations to the provided question. Here is an example:\n",
    "\n",
    "Question: What is the capital of Japan?\n",
    "\n",
    "Answer: Tokyo.\n",
    "\n",
    "Answer the user's prompt, delimited by {delimiter}\n",
    "\n",
    "\"\"\"}] # accumulate messages\n",
    "\n",
    "\n",
    "for index, row in test_df_SAMPLE.iterrows(): # 100 samples!\n",
    "    counter+=1\n",
    "    if counter%10==0:\n",
    "        print(counter)\n",
    "        # time.sleep(3)\n",
    "    user_input = row['ambiguous_question']\n",
    "    # print(\" -- User prompt: \", user_input)\n",
    "\n",
    "    # Input User's question\n",
    "    context.append({'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"})\n",
    "    response = get_completion_from_messages(context).lower()\n",
    "\n",
    "    # BLEU SCORE\n",
    "    response = remove_punctuation(response)\n",
    "    tokenized_response = get_tokenized_responses(response)\n",
    "    tokenized_actual = get_tokenized_responses(remove_punctuation(row['answer']))\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    # CITATION: Had to change this line of code to work with small answers. Help from ChatGPT.\n",
    "    bleu_score = sentence_bleu([tokenized_response], tokenized_actual, weights=(0.5, 0.5), smoothing_function=smooth_fn)\n",
    "    # print(tokenized_response,tokenized_actual)\n",
    "    # print(\"BLEU Score: \", bleu_score)\n",
    "    bleu_scores_array.append(bleu_score)\n",
    "\n",
    "    # METEOR SCORE\n",
    "    meteor_scored = meteor_score([tokenized_response], tokenized_actual)\n",
    "    # print(\"METEOR Score: \", meteor_scored)\n",
    "    meteor_scores_array.append(meteor_scored)\n",
    "    \n",
    "    context = context[:1] # reset the context array\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "# print(bleu_scores_array, meteor_scores_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90c2f872-9090-4399-8432-d3a0e5237d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15575274253547353\n",
      "0.02427511923077731\n"
     ]
    }
   ],
   "source": [
    "average_meteor_score = sum(meteor_scores_array) / len(meteor_scores_array)\n",
    "print(average_meteor_score)\n",
    "average_bleu_score = sum(bleu_scores_array) / len(bleu_scores_array)\n",
    "print(average_bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0a49b-d8df-45fd-8df2-8bce6d1c5d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
