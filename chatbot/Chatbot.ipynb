{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dd82db-5eb2-47bf-ad38-0507692c5255",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cddf8bf-a68e-47f4-965d-105a057b0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91371c3e-f091-439b-bc5c-1be2307548c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "sys.path.append('../..')\n",
    "import python_utils\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BertTokenizer, BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# from dotenv import loader_dotenv, find_dotenv\n",
    "\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c61168-fb2f-48d9-8b03-27c8fa132d0a",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0e592a-e0e6-472b-8c62-6fd0ab44f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function receives an array of 'messages' and returns an output based on this array.\n",
    "# INPUTS:\n",
    "#      * messages: an array of messages between user and gpt model.\n",
    "#      * TODO: complete\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo-16k\", temperature=0, max_tokens=500):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    last_message = response.choices[0].message\n",
    "    return last_message.content\n",
    "\n",
    "def get_prompt_ambig_category(user_prompt):\n",
    "    BERT_tokenizer = BertTokenizer.from_pretrained('../models/final_pretrained_saves_BERT')\n",
    "    BERT_model = BertForSequenceClassification.from_pretrained('../models/final_pretrained_saves_BERT')\n",
    "    BERT_model.eval()\n",
    "    \n",
    "    inputs = BERT_tokenizer(user_prompt, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = BERT_model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    query_classification = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    return query_classification\n",
    "\n",
    "\n",
    "def get_prompt_rephrase(ambiguous_prompt):\n",
    "    BART_tokenizer = BartTokenizer.from_pretrained('../models/final_pretrained_saves')\n",
    "    BART_model = BartForConditionalGeneration.from_pretrained('../models/final_pretrained_saves')    \n",
    "    BART_model.eval()\n",
    "    \n",
    "    inputs = BART_tokenizer.encode(ambiguous_prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = BART_model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    disambiguated_question = BART_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return disambiguated_question\n",
    "\n",
    "\n",
    "def get_tokenized_responses(response):\n",
    "    tokenized_response = response.split()\n",
    "    return tokenized_response\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136ca99-fca6-47e0-a473-2528506f6df4",
   "metadata": {},
   "source": [
    "## Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149c8da9-7aef-4b02-b63a-ce27d9e0b596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ambiguous_question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who was the heir apparent of the austro-hungar...</td>\n",
       "      <td>Archduke Franz Ferdinand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the heir apparent of the austro-hungar...</td>\n",
       "      <td>Archduke Karl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who sings don't get hooked on me?</td>\n",
       "      <td>Mac Davis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who sings don't get hooked on me?</td>\n",
       "      <td>Blaine Larson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who sings don't get hooked on me?</td>\n",
       "      <td>Liza Minelli</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ambiguous_question                    answer\n",
       "0  Who was the heir apparent of the austro-hungar...  Archduke Franz Ferdinand\n",
       "1  Who was the heir apparent of the austro-hungar...             Archduke Karl\n",
       "2                  Who sings don't get hooked on me?                 Mac Davis\n",
       "3                  Who sings don't get hooked on me?             Blaine Larson\n",
       "4                  Who sings don't get hooked on me?              Liza Minelli"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../../data/GPT/GPT_test_final_csv')\n",
    "test_df = test_df.drop(columns=['question_type', 'viewed_doc_titles', 'disambiguated_question'])\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1098d8a2-64c8-4e20-bb51-ff26eb4330e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919fba1-5275-47b2-8a46-da682d689f76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot with Ambiguous Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba39e4b2-587f-4176-9f01-59b374065bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Chatbot:  Hello! How can I assist you today? \n",
      "\n",
      "-- User: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What is the capital of USA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IS AMBIGUOUS?  1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_content_from_database' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIS AMBIGUOUS? \u001b[39m\u001b[38;5;124m\"\u001b[39m, is_ambiguous)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# conversation_history = [entry for entry in context if entry['role'] != 'system']\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# continuation_flag = int(is_continuation(conversation_history, user_input)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Match question with NQ Dataset\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m db_content \u001b[38;5;241m=\u001b[39m \u001b[43mget_content_from_database\u001b[49m(user_input)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_ambiguous \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQUESTION IS AMBIGUOUS!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_content_from_database' is not defined"
     ]
    }
   ],
   "source": [
    "panels = []\n",
    "delimiter = \"~~~~\"\n",
    "start_convo_flag = True\n",
    "\n",
    "context = [{'role': 'system', 'content':f\"\"\"\n",
    "\n",
    "You are a chatbot that for 'fact-based' or 'direct' questioning. You will be provided a question and you must provide an answer. \\\n",
    "You answer must be short. Do not include explanations to the provided question. Here is an example:\n",
    "\n",
    "Question: What is the capital of Japan?\n",
    "\n",
    "Answer: Tokyo.\n",
    "\n",
    "\n",
    "\"\"\"}] # accumulate messages\n",
    "\n",
    "\n",
    "context.append({'role':'user', 'content':f\"{delimiter}Hi!{delimiter}\"})\n",
    "init_response = get_completion_from_messages(context)\n",
    "\n",
    "print(\"-- Chatbot: \", init_response, \"\\n\")\n",
    "while start_convo_flag:\n",
    "    print(\"-- User: \")\n",
    "    user_input = input()\n",
    "    print()\n",
    "    if user_input == \"stop\":\n",
    "        break\n",
    "        \n",
    "    # Check if conversation is an ambiguous question or not\n",
    "    is_ambiguous = get_prompt_ambig_category(user_input)\n",
    "    print(\"IS AMBIGUOUS? \", is_ambiguous)\n",
    "    \n",
    "    # conversation_history = [entry for entry in context if entry['role'] != 'system']\n",
    "    # continuation_flag = int(is_continuation(conversation_history, user_input)\n",
    "    \n",
    "    # Match question with NQ Dataset\n",
    "    db_content = get_content_from_database(user_input)\n",
    "    \n",
    "    if is_ambiguous == 1:\n",
    "        print(\"QUESTION IS AMBIGUOUS!\")\n",
    "        rephrased_user_input = get_prompt_rephrase(user_input)\n",
    "        print(\"REPHRASED QUESTION: \", rephrased_user_input)\n",
    "        user_input = rephrased_user_input\n",
    "            \n",
    "    # Add System instructions\n",
    "    context.append({'role':'system', 'content': f\"\"\"Use the provided content to answer the question.\\\n",
    "                    Do not make assumptions or use information from your pre-trained data. Use this information: {db_content}\"\"\"})\n",
    "\n",
    "    # Input User's question\n",
    "    context.append({'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"})\n",
    "    response = get_completion_from_messages(context)\n",
    "    print(\"-- Chatbot: \", response, \"\\n\")\n",
    "    context.append({'role': 'assistant', 'content': f\"{response}\"})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d928b9f-d02c-46c2-a89b-d8b3757aea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- User: Who was the heir apparent of the austro-hungarian empire in 1914?\n",
      "\n",
      "QUESTION IS AMBIGUOUS!\n",
      "REPHRASED QUESTION:  Who was the heir apparent of the austro-hungarian empire in 1914?\n",
      " -- Chatbot: Archduke Franz Ferdinand.\n",
      " -- Expected Answer: Archduke Franz Ferdinand\n"
     ]
    }
   ],
   "source": [
    "panels = []\n",
    "bleu_scores = []\n",
    "meteor_scores = []\n",
    "bert_scores = []\n",
    "\n",
    "delimiter = \"~~~~\"\n",
    "\n",
    "context = [{'role': 'system', 'content':f\"\"\"\n",
    "\n",
    "You are a chatbot that for 'fact-based' or 'direct' questioning. You will be provided a question and you must provide an answer. \\\n",
    "You answer must be short. Do not include explanations to the provided question. Here is an example:\n",
    "\n",
    "Question: What is the capital of Japan?\n",
    "\n",
    "Answer: Tokyo.\n",
    "\n",
    "\n",
    "\"\"\"}] # accumulate messages\n",
    "\n",
    "\n",
    "# context.append({'role':'user', 'content':f\"{delimiter}Hi!{delimiter}\"})\n",
    "# init_response = get_completion_from_messages(context)\n",
    "\n",
    "\n",
    "# for index, row in test_df.iterrows():\n",
    "# user_input = row['ambiguous_question']\n",
    "\n",
    "user_input = test_df.iloc[0]['ambiguous_question']\n",
    "print(f\"-- User: {user_input}\\n\")\n",
    "\n",
    "# Check if conversation is an ambiguous question or not\n",
    "is_ambiguous = get_prompt_ambig_category(user_input)\n",
    "# print(\"IS AMBIGUOUS? \", is_ambiguous)\n",
    "\n",
    "\n",
    "if is_ambiguous == 1:\n",
    "    print(\"QUESTION IS AMBIGUOUS!\")\n",
    "    rephrased_user_input = get_prompt_rephrase(user_input)\n",
    "    print(\"REPHRASED QUESTION: \", rephrased_user_input)\n",
    "    user_input = rephrased_user_input\n",
    "\n",
    "# # Add System instructions\n",
    "# context.append({'role':'system', 'content': f\"\"\"Use the provided content to answer the question.\\\n",
    "#                 Do not make assumptions or use information from your pre-trained data. Use this information: {db_content}\"\"\"})\n",
    "\n",
    "# Input User's question\n",
    "context.append({'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"})\n",
    "response = get_completion_from_messages(context)\n",
    "context.append({'role': 'assistant', 'content': f\"{response}\"})\n",
    "print(f\" -- Chatbot: {response}\")\n",
    "\n",
    "print(f\" -- Expected Answer: {test_df.iloc[0]['answer']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c4c0775-d8a2-48c8-8fa1-ebea192e2910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archduke Franz Ferdinand\n",
      "['Archduke', 'Franz', 'Ferdinand']\n",
      "\n",
      "Archduke Franz Ferdinand\n",
      "['Archduke', 'Franz', 'Ferdinand']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "response = remove_punctuation(response)\n",
    "\n",
    "tokenized_response = get_tokenized_responses(response)\n",
    "tokenized_actual = get_tokenized_responses(test_df.iloc[0]['answer'])\n",
    "print(response)\n",
    "print(tokenized_response)\n",
    "print()\n",
    "print(test_df.iloc[0]['answer'])\n",
    "print(tokenized_actual)\n",
    "\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "# CITATION: Had to change this line of code to work with small answers. Help from ChatGPT.\n",
    "bleu_score = sentence_bleu([tokenized_response], tokenized_actual, weights=(0.5, 0.5), smoothing_function=smooth_fn)\n",
    "\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "865cab0b-b3fd-4a1f-9798-1e78295edae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Archduke', 'Franz', 'Ferdinand']\n",
      "['Archduke', 'Franz', 'Ferdinand']\n",
      "0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_response)\n",
    "print(tokenized_actual)\n",
    "\n",
    "meteor_scores = meteor_score([tokenized_response], tokenized_actual)\n",
    "print(meteor_scores)\n",
    "\n",
    "# average_meteor_score = sum(meteor_scores) / len(meteor_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb368f-4002-4728-a6cd-d7bfab3e517e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a31bf7-d3f8-4a31-b133-7068b1166454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - Change script to test on the test AmbigNQ data\n",
    "# - Prompt-engineer GPT model to output short answers.\n",
    "# - Extract each ChatGPT output and Compare answers using embedding model and thus euclidean similarity?\n",
    "\n",
    "# Potential Improvements\n",
    "# - Hallucination tracking\n",
    "# - RAG system with GoogleNQ dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988538a-b781-4fea-831d-c46630bba684",
   "metadata": {},
   "source": [
    "## Chatbot With Ambiguous Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc1a37-741b-4545-8c8b-f93e879e4a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c69975-3c55-4c06-9774-4e56ea1a8079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
