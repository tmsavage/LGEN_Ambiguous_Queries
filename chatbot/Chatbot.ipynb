{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dd82db-5eb2-47bf-ad38-0507692c5255",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cddf8bf-a68e-47f4-965d-105a057b0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91371c3e-f091-439b-bc5c-1be2307548c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "sys.path.append('../..')\n",
    "import python_utils\n",
    "import torch\n",
    "import ast\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BertTokenizer, BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# from dotenv import loader_dotenv, find_dotenv\n",
    "\n",
    "API_KEY = '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'\n",
    "client = OpenAI(api_key = API_KEY)\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c61168-fb2f-48d9-8b03-27c8fa132d0a",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d0e592a-e0e6-472b-8c62-6fd0ab44f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function receives an array of 'messages' and returns an output based on this array.\n",
    "# INPUTS:\n",
    "#      * messages: an array of messages between user and gpt model.\n",
    "#      * TODO: complete\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo-16k\", temperature=0, max_tokens=500):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    last_message = response.choices[0].message\n",
    "    return last_message.content\n",
    "\n",
    "def get_prompt_ambig_category(user_prompt):\n",
    "    BERT_tokenizer = BertTokenizer.from_pretrained('../models/final_pretrained_saves_BERT')\n",
    "    BERT_model = BertForSequenceClassification.from_pretrained('../models/final_pretrained_saves_BERT')\n",
    "    BERT_model.eval()\n",
    "    \n",
    "    inputs = BERT_tokenizer(user_prompt, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = BERT_model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    query_classification = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    return query_classification\n",
    "\n",
    "\n",
    "def get_prompt_rephrase(ambiguous_prompt):\n",
    "    BART_tokenizer = BartTokenizer.from_pretrained('../models/final_pretrained_saves')\n",
    "    BART_model = BartForConditionalGeneration.from_pretrained('../models/final_pretrained_saves')    \n",
    "    BART_model.eval()\n",
    "    \n",
    "    inputs = BART_tokenizer.encode(ambiguous_prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = BART_model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    disambiguated_question = BART_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return disambiguated_question\n",
    "\n",
    "\n",
    "def get_tokenized_responses(response):\n",
    "    tokenized_response = response.split()\n",
    "    return tokenized_response\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def embed_user_prompt(user_prompt, model=\"text-embedding-ada-002\"):\n",
    "    \n",
    "    # try:\n",
    "    #     # Create the embedding\n",
    "    #     response = client.embeddings.create(input=[user_prompt], model=model)\n",
    "    #     embedding = response.data[0].embedding\n",
    "    #     return embedding\n",
    "    # except Exception as e:\n",
    "    #     # Handle exceptions\n",
    "    #     print(f\"An error occurred: {e}\")\n",
    "    #     return None\n",
    "    \n",
    "    response = client.embeddings.create(input=[user_prompt], model=model)\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding\n",
    "    \n",
    "\n",
    "# TODO\n",
    "def get_content_from_database(embedded_user_prompt, df):\n",
    "    print(type(embedded_user_prompt))\n",
    "    \n",
    "    embedded_user_prompt = embedded_user_prompt.reshape(1, -1)\n",
    "    page_embeddings = np.vstack(df['page_embedding'])\n",
    "    print(type(page_embeddings))\n",
    "    similarities = cosine_similarity(embedded_user_prompt, page_embeddings)\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "\n",
    "    return df.iloc[most_similar_index]['page_content']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136ca99-fca6-47e0-a473-2528506f6df4",
   "metadata": {},
   "source": [
    "## Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "149c8da9-7aef-4b02-b63a-ce27d9e0b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../../data/GPT/GPT_test_final_csv')\n",
    "test_df = test_df.drop(columns=['question_type', 'viewed_doc_titles', 'disambiguated_question'])\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820d6ca9-b710-4d56-9862-0cb7ceb40e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(test_df['page_content'])\n",
    "test_df = test_df.drop(columns=['page_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb1e69b-ceb6-4839-bf24-5013a1c864c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>~~In chemistry, a mixture is a material made u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        page_content\n",
       "0  ~~The Constitution of the State of Texas is th...\n",
       "1  ~~The Constitution of the State of Texas is th...\n",
       "2  ~~The Man Who Shot Liberty Valance is a 1962 A...\n",
       "3  ~~The Man Who Shot Liberty Valance is a 1962 A...\n",
       "4  ~~In chemistry, a mixture is a material made u..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CITATION: https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "embeddings_df['page_content'] = embeddings_df['page_content'].apply(remove_html_tags)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77bdae49-f108-4f87-abea-d62d6e0ee55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n"
     ]
    }
   ],
   "source": [
    "# embedding_model = \"text-embedding-ada-002\"\n",
    "# embeddings = []\n",
    "\n",
    "# # CITATION: https://platform.openai.com/docs/guides/embeddings/use-cases\n",
    "# \n",
    "# def get_embedding(text, model=\"text-embedding-ada-002\", max_length = 8000):\n",
    "#     # text = text.replace(\"\\n\", \" \")[:max_length]\n",
    "#     # return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "#     text = text.replace(\"\\n\", \" \")[:max_length]\n",
    "#     try:\n",
    "#         return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error occurred: {e}\")\n",
    "#         time.sleep(10)  # Wait for 10 seconds before retrying\n",
    "#         return None\n",
    "    \n",
    "# embeddings = []\n",
    "# counter = 0\n",
    "# for row in embeddings_df['page_content']:\n",
    "#     if counter%200==0:\n",
    "#         print(counter)\n",
    "        \n",
    "#     embedding = get_embedding(row, model='text-embedding-ada-002')\n",
    "#     embeddings.append(embedding)\n",
    "#     counter+=1\n",
    "\n",
    "# embeddings_df['page_embedding'] = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3cc67a5-9199-46ef-99b0-325328684005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_df.to_csv('embedded_page_contents.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0d96550-60b2-4f57-a87c-b17d17b04a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_csv('embedded_page_contents.csv')\n",
    "embeddings_df['page_embedding'] = embeddings_df['page_embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2aadaa6e-2f5e-490d-a12d-aedc5efb6ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_content</th>\n",
       "      <th>page_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "      <td>[-0.0027238428592681885, 0.020244302228093147,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "      <td>[-0.0027419731486588717, 0.020274952054023743,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "      <td>[-0.020264700055122375, -0.018238229677081108,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "      <td>[-0.020236164331436157, -0.0182228721678257, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>~~In chemistry, a mixture is a material made u...</td>\n",
       "      <td>[0.006561756134033203, 0.02816181816160679, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        page_content  \\\n",
       "0  ~~The Constitution of the State of Texas is th...   \n",
       "1  ~~The Constitution of the State of Texas is th...   \n",
       "2  ~~The Man Who Shot Liberty Valance is a 1962 A...   \n",
       "3  ~~The Man Who Shot Liberty Valance is a 1962 A...   \n",
       "4  ~~In chemistry, a mixture is a material made u...   \n",
       "\n",
       "                                      page_embedding  \n",
       "0  [-0.0027238428592681885, 0.020244302228093147,...  \n",
       "1  [-0.0027419731486588717, 0.020274952054023743,...  \n",
       "2  [-0.020264700055122375, -0.018238229677081108,...  \n",
       "3  [-0.020236164331436157, -0.0182228721678257, 0...  \n",
       "4  [0.006561756134033203, 0.02816181816160679, -0...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919fba1-5275-47b2-8a46-da682d689f76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot with Ambiguous Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba39e4b2-587f-4176-9f01-59b374065bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# panels = []\n",
    "# delimiter = \"~~~~\"\n",
    "# start_convo_flag = True\n",
    "\n",
    "# context = [{'role': 'system', 'content':f\"\"\"\n",
    "\n",
    "# You are a chatbot that for 'fact-based' or 'direct' questioning. You will be provided a question and you must provide an answer. \\\n",
    "# You answer must be short. Do not include explanations to the provided question. Here is an example:\n",
    "\n",
    "# Question: What is the capital of Japan?\n",
    "\n",
    "# Answer: Tokyo.\n",
    "\n",
    "\n",
    "# \"\"\"}] # accumulate messages\n",
    "\n",
    "\n",
    "# context.append({'role':'user', 'content':f\"{delimiter}Hi!{delimiter}\"})\n",
    "# init_response = get_completion_from_messages(context)\n",
    "\n",
    "# print(\"-- Chatbot: \", init_response, \"\\n\")\n",
    "# while start_convo_flag:\n",
    "#     print(\"-- User: \")\n",
    "#     user_input = input()\n",
    "#     print()\n",
    "#     if user_input == \"stop\":\n",
    "#         break\n",
    "        \n",
    "#     # Check if conversation is an ambiguous question or not\n",
    "#     is_ambiguous = get_prompt_ambig_category(user_input)\n",
    "#     print(\"IS AMBIGUOUS? \", is_ambiguous)\n",
    "    \n",
    "#     # conversation_history = [entry for entry in context if entry['role'] != 'system']\n",
    "#     # continuation_flag = int(is_continuation(conversation_history, user_input)\n",
    "    \n",
    "#     # Match question with NQ Dataset\n",
    "#     db_content = get_content_from_database(user_input)\n",
    "    \n",
    "#     if is_ambiguous == 1:\n",
    "#         print(\"QUESTION IS AMBIGUOUS!\")\n",
    "#         rephrased_user_input = get_prompt_rephrase(user_input)\n",
    "#         print(\"REPHRASED QUESTION: \", rephrased_user_input)\n",
    "#         user_input = rephrased_user_input\n",
    "            \n",
    "#     # Add System instructions\n",
    "#     context.append({'role':'system', 'content': f\"\"\"Use the provided content to answer the question.\\\n",
    "#                     Do not make assumptions or use information from your pre-trained data. Use this information: {db_content}\"\"\"})\n",
    "\n",
    "#     # Input User's question\n",
    "#     context.append({'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"})\n",
    "#     response = get_completion_from_messages(context)\n",
    "#     print(\"-- Chatbot: \", response, \"\\n\")\n",
    "#     context.append({'role': 'assistant', 'content': f\"{response}\"})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d928b9f-d02c-46c2-a89b-d8b3757aea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- User: The first article of the texas constitution concerns?\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "~~The Constitution of the State of Texas is the document that describes the structure \n",
      "and function of the government of the U.S. state of Texas. The current&nbsp;...~~The Constitution of the Republic of Texas was the supreme law of Texas from \n",
      "1836 to 1845. On March 2, 1836, Texas declared itself an independent republic&nbsp;...~~Proposition 2 was a referendum for a state constitutional amendment placed on \n",
      "the ballot by the Texas legislature and approved by the voters at the November 8\n",
      "&nbsp;...~~Article X of the Texas Constitution of 1876 covers railroad companies and the \n",
      "creation of the Railroad Commission of Texas. The federal government later&nbsp;...~~The Texas Constitution sets the qualifications for election to each house as \n",
      "follows: A senator must be at least 26 years of age, a resident of Texas for five \n",
      "years&nbsp;...~~Federal constitutional process[edit]. Article IV, Section 3, of The United States \n",
      "Constitution expressly prohibits any&nbsp;...~~Rodríguez filed a bill in the Texas Legislature to formally abolish the state&#39;s ban \n",
      "on same-sex marriage. Constitution[edit]. On&nbsp;...~~Humbly invoking the blessings of Almighty God, the people of the State of Texas \n",
      "do ordain and establish this Constitution. Article 1: Bill of Rights[edit]. That the&nbsp;...~~Burnet and developed a Texas Constitution, which they based primarily on the \n",
      "Constitution of the United States. On March 6 they received a missive from the&nbsp;...~~The Texas constable is provided for in the Texas Constitution of 1876 (Article 5, \n",
      "Section 18), which calls for the election of a constable in each Texas precinct of&nbsp;...~~\n",
      " -- Chatbot: bill of rights.\n",
      " -- Expected Answer: bill of rights\n"
     ]
    }
   ],
   "source": [
    "panels = []\n",
    "bleu_scores = []\n",
    "meteor_scores = []\n",
    "bert_scores = []\n",
    "\n",
    "delimiter = \"~~~~\"\n",
    "\n",
    "context = [{'role': 'system', 'content':f\"\"\"\n",
    "\n",
    "You are a chatbot that for 'fact-based' or 'direct' questioning. You will be provided a question and you must provide an answer. \\\n",
    "You answer must be short. Do not include explanations to the provided question. Here is an example:\n",
    "\n",
    "Question: What is the capital of Japan?\n",
    "\n",
    "Answer: Tokyo.\n",
    "\n",
    "\n",
    "\"\"\"}] # accumulate messages\n",
    "\n",
    "\n",
    "# context.append({'role':'user', 'content':f\"{delimiter}Hi!{delimiter}\"})\n",
    "# init_response = get_completion_from_messages(context)\n",
    "\n",
    "\n",
    "# for index, row in test_df.iterrows():\n",
    "# user_input = row['ambiguous_question']\n",
    "\n",
    "user_input = test_df.iloc[0]['ambiguous_question']\n",
    "print(f\"-- User: {user_input}\\n\")\n",
    "\n",
    "# Check if conversation is an ambiguous question or not\n",
    "is_ambiguous = get_prompt_ambig_category(user_input)\n",
    "# print(\"IS AMBIGUOUS? \", is_ambiguous)\n",
    "\n",
    "if is_ambiguous == 1:\n",
    "    print(\"QUESTION IS AMBIGUOUS!\")\n",
    "    rephrased_user_input = get_prompt_rephrase(user_input)\n",
    "    print(\"REPHRASED QUESTION: \", rephrased_user_input)\n",
    "    user_input = rephrased_user_input\n",
    "    \n",
    "embedded_user_prompt =np.array(embed_user_prompt(user_input))\n",
    "\n",
    "db_page_contents = get_content_from_database(embedded_user_prompt, embeddings_df)\n",
    "print(db_page_contents)\n",
    "\n",
    "Add System instructions\n",
    "context.append({'role':'system', 'content': f\"\"\"Use the provided content to answer the question.\\\n",
    "                Do not make assumptions or use information from your pre-trained data. Use this information: {db_page_content}\"\"\"})\n",
    "\n",
    "# Input User's question\n",
    "context.append({'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"})\n",
    "response = get_completion_from_messages(context).lower()\n",
    "context.append({'role': 'assistant', 'content': f\"{response}\"})\n",
    "print(f\" -- Chatbot: {response}\")\n",
    "\n",
    "print(f\" -- Expected Answer: {test_df.iloc[0]['answer']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c4c0775-d8a2-48c8-8fa1-ebea192e2910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill of rights\n",
      "['bill', 'of', 'rights']\n",
      "\n",
      "bill of rights\n",
      "['bill', 'of', 'rights']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "response = remove_punctuation(response)\n",
    "\n",
    "tokenized_response = get_tokenized_responses(response)\n",
    "tokenized_actual = get_tokenized_responses(test_df.iloc[0]['answer'])\n",
    "print(response)\n",
    "print(tokenized_response)\n",
    "print()\n",
    "print(test_df.iloc[0]['answer'])\n",
    "print(tokenized_actual)\n",
    "\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "# CITATION: Had to change this line of code to work with small answers. Help from ChatGPT.\n",
    "bleu_score = sentence_bleu([tokenized_response], tokenized_actual, weights=(0.5, 0.5), smoothing_function=smooth_fn)\n",
    "\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "865cab0b-b3fd-4a1f-9798-1e78295edae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bill', 'of', 'rights']\n",
      "['bill', 'of', 'rights']\n",
      "0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_response)\n",
    "print(tokenized_actual)\n",
    "\n",
    "meteor_scores = meteor_score([tokenized_response], tokenized_actual)\n",
    "print(meteor_scores)\n",
    "\n",
    "# average_meteor_score = sum(meteor_scores) / len(meteor_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb368f-4002-4728-a6cd-d7bfab3e517e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a31bf7-d3f8-4a31-b133-7068b1166454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - Change script to test on the test AmbigNQ data\n",
    "# - Prompt-engineer GPT model to output short answers.\n",
    "# - Extract each ChatGPT output and Compare answers using embedding model and thus euclidean similarity?\n",
    "\n",
    "# Potential Improvements\n",
    "# - Hallucination tracking\n",
    "# - RAG system with GoogleNQ dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988538a-b781-4fea-831d-c46630bba684",
   "metadata": {},
   "source": [
    "## Chatbot With Ambiguous Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc1a37-741b-4545-8c8b-f93e879e4a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c69975-3c55-4c06-9774-4e56ea1a8079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
