{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dd82db-5eb2-47bf-ad38-0507692c5255",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cddf8bf-a68e-47f4-965d-105a057b0f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-utils\n",
      "  Obtaining dependency information for python-utils from https://files.pythonhosted.org/packages/f0/7b/e83e7b184e53530abe064b237a3731c738d3cb59f4201f3ce1a4ec0efe6f/python_utils-3.8.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading python_utils-3.8.1-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: typing-extensions>3.10.0.2 in /opt/conda/lib/python3.10/site-packages (from python-utils) (4.8.0)\n",
      "Downloading python_utils-3.8.1-py2.py3-none-any.whl (27 kB)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: python-utils\n",
      "Successfully installed python-utils-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91371c3e-f091-439b-bc5c-1be2307548c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "sys.path.append('../..')\n",
    "import python_utils\n",
    "# from dotenv import loader_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c61168-fb2f-48d9-8b03-27c8fa132d0a",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d0e592a-e0e6-472b-8c62-6fd0ab44f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function receives an array of 'messages' and returns an output based on this array.\n",
    "# INPUTS:\n",
    "#      * messages: an array of messages between user and gpt model.\n",
    "#      * TODO: complete\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo-16k\", temperature=0, max_tokens=500):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "# TODO: Complete this funciton\n",
    "def get_prompt_category(user_prompt):\n",
    "    return None\n",
    "\n",
    "\n",
    "# TODO: Complete this funciton\n",
    "def get_content_from_database(user_prompt):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919fba1-5275-47b2-8a46-da682d689f76",
   "metadata": {},
   "source": [
    "## Chatbot without Ambiguous Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba39e4b2-587f-4176-9f01-59b374065bbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m context \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mJust be a normal chatbot.\u001b[39m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m}] \u001b[38;5;66;03m# accumulate messages\u001b[39;00m\n\u001b[1;32m     12\u001b[0m context\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelimiter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mHi!\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelimiter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m---> 13\u001b[0m init_response \u001b[38;5;241m=\u001b[39m \u001b[43mget_completion_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-- Chatbot: \u001b[39m\u001b[38;5;124m\"\u001b[39m, init_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start_convo_flag:\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mget_completion_from_messages\u001b[0;34m(messages, model, temperature, max_tokens)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_completion_from_messages\u001b[39m(messages, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-16k\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      7\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      8\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m      9\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     10\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/openai/_utils/_proxy.py:25\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     proxied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_proxied__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxied, LazyProxy):\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m proxied  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/openai/_utils/_proxy.py:61\u001b[0m, in \u001b[0;36mLazyProxy.__get_proxied__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get_proxied__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_cache:\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     proxied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__proxied\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proxied \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/openai/_module_client.py:12\u001b[0m, in \u001b[0;36mChatProxy.__load__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__load__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m resources\u001b[38;5;241m.\u001b[39mChat:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchat\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/openai/__init__.py:312\u001b[0m, in \u001b[0;36m_load_client\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         _client \u001b[38;5;241m=\u001b[39m _AzureModuleClient(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    297\u001b[0m             api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[1;32m    298\u001b[0m             azure_endpoint\u001b[38;5;241m=\u001b[39mazure_endpoint,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m             http_client\u001b[38;5;241m=\u001b[39mhttp_client,\n\u001b[1;32m    309\u001b[0m         )\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[0;32m--> 312\u001b[0m     _client \u001b[38;5;241m=\u001b[39m \u001b[43m_ModuleClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _client\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/openai/_client.py:93\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     91\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "panels = []\n",
    "delimiter = \"~~~~\"\n",
    "start_convo_flag = True\n",
    "\n",
    "context = [{'role': 'system', 'content':f\"\"\"\n",
    "\n",
    "Just be a normal chatbot.\n",
    "\n",
    "\"\"\"}] # accumulate messages\n",
    "\n",
    "\n",
    "context.append({'role':'user', 'content':f\"{delimiter}Hi!{delimiter}\"})\n",
    "init_response = get_completion_from_messages(context)\n",
    "\n",
    "print(\"-- Chatbot: \", init_response, \"\\n\")\n",
    "while start_convo_flag:\n",
    "    print(\"-- User: \")\n",
    "    user_input = input()\n",
    "    print()\n",
    "    if user_input == \"stop\":\n",
    "        break\n",
    "        \n",
    "    # Check if conversation is a continuation of a particular topic\n",
    "    # conversation_history = [entry for entry in context if entry['role'] != 'system']\n",
    "    # continuation_flag = int(is_continuation(conversation_history, user_input)\n",
    "    \n",
    "    # Match question with NQ Dataset\n",
    "    db_content = get_content_from_database(user_input)\n",
    "    \n",
    "    # Add System instructions\n",
    "    context.append({'role':'system', 'content': f\"\"\"Use the provided content to answer the question.\\\n",
    "                    Do not make assumptions or use information from your pre-trained data. Use this information: {db_content}\"\"\"})\n",
    "    \n",
    "    # Input User's question\n",
    "    context.append({'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"})\n",
    "    response = get_completion_from_messages(context)\n",
    "    print(\"-- Chatbot: \", response, \"\\n\")\n",
    "    context.append({'role': 'assistant', 'content': f\"{response}\"})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a31bf7-d3f8-4a31-b133-7068b1166454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c988538a-b781-4fea-831d-c46630bba684",
   "metadata": {},
   "source": [
    "## Chatbot With Ambiguous Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc1a37-741b-4545-8c8b-f93e879e4a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c69975-3c55-4c06-9774-4e56ea1a8079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
