{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dd82db-5eb2-47bf-ad38-0507692c5255",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cddf8bf-a68e-47f4-965d-105a057b0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91371c3e-f091-439b-bc5c-1be2307548c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "sys.path.append('../..')\n",
    "import python_utils\n",
    "import torch\n",
    "import ast\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BertTokenizer, BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# from dotenv import loader_dotenv, find_dotenv\n",
    "\n",
    "API_KEY = ''\n",
    "client = OpenAI(api_key = API_KEY)\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c61168-fb2f-48d9-8b03-27c8fa132d0a",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0e592a-e0e6-472b-8c62-6fd0ab44f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function receives an array of 'messages' and returns an output based on this array.\n",
    "# INPUTS:\n",
    "#      * messages: an array of messages between user and gpt model.\n",
    "#      * TODO: complete\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo-16k\", temperature=0, max_tokens=500):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    last_message = response.choices[0].message\n",
    "    return last_message.content\n",
    "\n",
    "def get_prompt_ambig_category(user_prompt):\n",
    "    BERT_tokenizer = BertTokenizer.from_pretrained('../models/final_pretrained_saves_BERT')\n",
    "    BERT_model = BertForSequenceClassification.from_pretrained('../models/final_pretrained_saves_BERT')\n",
    "    BERT_model.eval()\n",
    "    \n",
    "    inputs = BERT_tokenizer(user_prompt, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = BERT_model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    query_classification = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    return query_classification\n",
    "\n",
    "\n",
    "def get_prompt_rephrase(ambiguous_prompt):\n",
    "    BART_tokenizer = BartTokenizer.from_pretrained('../models/final_pretrained_saves')\n",
    "    BART_model = BartForConditionalGeneration.from_pretrained('../models/final_pretrained_saves')    \n",
    "    BART_model.eval()\n",
    "    \n",
    "    inputs = BART_tokenizer.encode(ambiguous_prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = BART_model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    disambiguated_question = BART_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return disambiguated_question\n",
    "\n",
    "\n",
    "def get_tokenized_responses(response):\n",
    "    tokenized_response = response.split()\n",
    "    return tokenized_response\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def embed_user_prompt(user_prompt, model=\"text-embedding-ada-002\"):\n",
    "    \n",
    "    # try:\n",
    "    #     # Create the embedding\n",
    "    #     response = client.embeddings.create(input=[user_prompt], model=model)\n",
    "    #     embedding = response.data[0].embedding\n",
    "    #     return embedding\n",
    "    # except Exception as e:\n",
    "    #     # Handle exceptions\n",
    "    #     print(f\"An error occurred: {e}\")\n",
    "    #     return None\n",
    "    \n",
    "    response = client.embeddings.create(input=[user_prompt], model=model)\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding\n",
    "    \n",
    "\n",
    "# TODO\n",
    "def get_content_from_database(embedded_user_prompt, df):\n",
    "    \n",
    "    embedded_user_prompt = embedded_user_prompt.reshape(1, -1)\n",
    "    page_embeddings = np.vstack(df['page_embedding'])\n",
    "    similarities = cosine_similarity(embedded_user_prompt, page_embeddings)\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "\n",
    "    return df.iloc[most_similar_index]['page_content']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136ca99-fca6-47e0-a473-2528506f6df4",
   "metadata": {},
   "source": [
    "## Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "149c8da9-7aef-4b02-b63a-ce27d9e0b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../../data/GPT/GPT_test_final_csv')\n",
    "test_df = test_df.drop(columns=['question_type', 'viewed_doc_titles', 'disambiguated_question'])\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820d6ca9-b710-4d56-9862-0cb7ceb40e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(test_df['page_content'])\n",
    "test_df = test_df.drop(columns=['page_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb1e69b-ceb6-4839-bf24-5013a1c864c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>~~In chemistry, a mixture is a material made u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        page_content\n",
       "0  ~~The Constitution of the State of Texas is th...\n",
       "1  ~~The Constitution of the State of Texas is th...\n",
       "2  ~~The Man Who Shot Liberty Valance is a 1962 A...\n",
       "3  ~~The Man Who Shot Liberty Valance is a 1962 A...\n",
       "4  ~~In chemistry, a mixture is a material made u..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CITATION: https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "embeddings_df['page_content'] = embeddings_df['page_content'].apply(remove_html_tags)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77bdae49-f108-4f87-abea-d62d6e0ee55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n"
     ]
    }
   ],
   "source": [
    "# embedding_model = \"text-embedding-ada-002\"\n",
    "# embeddings = []\n",
    "\n",
    "# # CITATION: https://platform.openai.com/docs/guides/embeddings/use-cases\n",
    "# \n",
    "# def get_embedding(text, model=\"text-embedding-ada-002\", max_length = 8000):\n",
    "#     # text = text.replace(\"\\n\", \" \")[:max_length]\n",
    "#     # return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "#     text = text.replace(\"\\n\", \" \")[:max_length]\n",
    "#     try:\n",
    "#         return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error occurred: {e}\")\n",
    "#         time.sleep(10)  # Wait for 10 seconds before retrying\n",
    "#         return None\n",
    "    \n",
    "# embeddings = []\n",
    "# counter = 0\n",
    "# for row in embeddings_df['page_content']:\n",
    "#     if counter%200==0:\n",
    "#         print(counter)\n",
    "        \n",
    "#     embedding = get_embedding(row, model='text-embedding-ada-002')\n",
    "#     embeddings.append(embedding)\n",
    "#     counter+=1\n",
    "\n",
    "# embeddings_df['page_embedding'] = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3cc67a5-9199-46ef-99b0-325328684005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_df.to_csv('embedded_page_contents.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0d96550-60b2-4f57-a87c-b17d17b04a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_csv('embedded_page_contents.csv')\n",
    "embeddings_df['page_embedding'] = embeddings_df['page_embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aadaa6e-2f5e-490d-a12d-aedc5efb6ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_content</th>\n",
       "      <th>page_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "      <td>[-0.0027238428592681885, 0.020244302228093147,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>~~The Constitution of the State of Texas is th...</td>\n",
       "      <td>[-0.0027419731486588717, 0.020274952054023743,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "      <td>[-0.020264700055122375, -0.018238229677081108,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>~~The Man Who Shot Liberty Valance is a 1962 A...</td>\n",
       "      <td>[-0.020236164331436157, -0.0182228721678257, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>~~In chemistry, a mixture is a material made u...</td>\n",
       "      <td>[0.006561756134033203, 0.02816181816160679, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        page_content  \\\n",
       "0  ~~The Constitution of the State of Texas is th...   \n",
       "1  ~~The Constitution of the State of Texas is th...   \n",
       "2  ~~The Man Who Shot Liberty Valance is a 1962 A...   \n",
       "3  ~~The Man Who Shot Liberty Valance is a 1962 A...   \n",
       "4  ~~In chemistry, a mixture is a material made u...   \n",
       "\n",
       "                                      page_embedding  \n",
       "0  [-0.0027238428592681885, 0.020244302228093147,...  \n",
       "1  [-0.0027419731486588717, 0.020274952054023743,...  \n",
       "2  [-0.020264700055122375, -0.018238229677081108,...  \n",
       "3  [-0.020236164331436157, -0.0182228721678257, 0...  \n",
       "4  [0.006561756134033203, 0.02816181816160679, -0...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03be04ea-6f22-4b09-a1c1-46d41a97681b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ambiguous_question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>What is the classification of a loggerhead sea...</td>\n",
       "      <td>Testudines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>Where will the super rugby final be played?</td>\n",
       "      <td>Ellis Park Stadium, Johannesburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>Who controls the house of representatives righ...</td>\n",
       "      <td>The Democratic Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>Who is opening for luke bryan kill the lights ...</td>\n",
       "      <td>Brett Eldredge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Who hit the longest drive in golf history?</td>\n",
       "      <td>Mike Dobbyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ambiguous_question  \\\n",
       "596   What is the classification of a loggerhead sea...   \n",
       "4507        Where will the super rugby final be played?   \n",
       "3049  Who controls the house of representatives righ...   \n",
       "2957  Who is opening for luke bryan kill the lights ...   \n",
       "8            Who hit the longest drive in golf history?   \n",
       "\n",
       "                                answer  \n",
       "596                         Testudines  \n",
       "4507  Ellis Park Stadium, Johannesburg  \n",
       "3049              The Democratic Party  \n",
       "2957                    Brett Eldredge  \n",
       "8                          Mike Dobbyn  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_SAMPLE = test_df.sample(n=200, random_state=42)\n",
    "test_df_SAMPLE.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919fba1-5275-47b2-8a46-da682d689f76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot with Ambiguous Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba39e4b2-587f-4176-9f01-59b374065bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# panels = []\n",
    "# delimiter = \"~~~~\"\n",
    "# start_convo_flag = True\n",
    "\n",
    "# context = [{'role': 'system', 'content':f\"\"\"\n",
    "\n",
    "# You are a chatbot that for 'fact-based' or 'direct' questioning. You will be provided a question and you must provide an answer. \\\n",
    "# You answer must be short. Do not include explanations to the provided question. Here is an example:\n",
    "\n",
    "# Question: What is the capital of Japan?\n",
    "\n",
    "# Answer: Tokyo.\n",
    "\n",
    "\n",
    "# \"\"\"}] # accumulate messages\n",
    "\n",
    "\n",
    "# context.append({'role':'user', 'content':f\"{delimiter}Hi!{delimiter}\"})\n",
    "# init_response = get_completion_from_messages(context)\n",
    "\n",
    "# print(\"-- Chatbot: \", init_response, \"\\n\")\n",
    "# while start_convo_flag:\n",
    "#     print(\"-- User: \")\n",
    "#     user_input = input()\n",
    "#     print()\n",
    "#     if user_input == \"stop\":\n",
    "#         break\n",
    "        \n",
    "#     # Check if conversation is an ambiguous question or not\n",
    "#     is_ambiguous = get_prompt_ambig_category(user_input)\n",
    "#     print(\"IS AMBIGUOUS? \", is_ambiguous)\n",
    "    \n",
    "#     # conversation_history = [entry for entry in context if entry['role'] != 'system']\n",
    "#     # continuation_flag = int(is_continuation(conversation_history, user_input)\n",
    "    \n",
    "#     # Match question with NQ Dataset\n",
    "#     db_content = get_content_from_database(user_input)\n",
    "    \n",
    "#     if is_ambiguous == 1:\n",
    "#         print(\"QUESTION IS AMBIGUOUS!\")\n",
    "#         rephrased_user_input = get_prompt_rephrase(user_input)\n",
    "#         print(\"REPHRASED QUESTION: \", rephrased_user_input)\n",
    "#         user_input = rephrased_user_input\n",
    "            \n",
    "#     # Add System instructions\n",
    "#     context.append({'role':'system', 'content': f\"\"\"Use the provided content to answer the question.\\\n",
    "#                     Do not make assumptions or use information from your pre-trained data. Use this information: {db_content}\"\"\"})\n",
    "\n",
    "#     # Input User's question\n",
    "#     context.append({'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"})\n",
    "#     response = get_completion_from_messages(context)\n",
    "#     print(\"-- Chatbot: \", response, \"\\n\")\n",
    "#     context.append({'role': 'assistant', 'content': f\"{response}\"})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d928b9f-d02c-46c2-a89b-d8b3757aea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "panels = []\n",
    "bleu_scores_array = []\n",
    "meteor_scores_array = []\n",
    "bert_scores = []\n",
    "counter = 0\n",
    "\n",
    "delimiter = \"~~~~\"\n",
    "\n",
    "context = [{'role': 'system', 'content':f\"\"\"\n",
    "\n",
    "You are a chatbot that for 'fact-based' or 'direct' questioning. You will be provided a question and you must provide an answer. \\\n",
    "You answer must be short. Do not include explanations to the provided question. Here is an example:\n",
    "\n",
    "Question: What is the capital of Japan?\n",
    "\n",
    "Answer: Tokyo.\n",
    "\n",
    "\n",
    "\"\"\"}] # accumulate messages\n",
    "\n",
    "\n",
    "# context.append({'role':'user', 'content':f\"{delimiter}Hi!{delimiter}\"})\n",
    "# init_response = get_completion_from_messages(context)\n",
    "\n",
    "\n",
    "for index, row in test_df_SAMPLE.iterrows(): # 200 samples!\n",
    "    counter+=1\n",
    "    if counter%10==0:\n",
    "        print(counter)\n",
    "    user_input = row['ambiguous_question']\n",
    "\n",
    "\n",
    "    # Check if conversation is an ambiguous question or not\n",
    "    is_ambiguous = get_prompt_ambig_category(user_input)\n",
    "    # print(\"IS AMBIGUOUS? \", is_ambiguous)\n",
    "\n",
    "    if is_ambiguous == 1:\n",
    "        # print(\"QUESTION IS AMBIGUOUS!\")\n",
    "        rephrased_user_input = get_prompt_rephrase(user_input)\n",
    "        # print(\"REPHRASED QUESTION: \", rephrased_user_input)\n",
    "        user_input = rephrased_user_input\n",
    "\n",
    "    embedded_user_prompt =np.array(embed_user_prompt(user_input))\n",
    "\n",
    "    db_page_contents = get_content_from_database(embedded_user_prompt, embeddings_df)\n",
    "\n",
    "    # Add System instructions\n",
    "    context.append({'role':'system', 'content': f\"\"\"Use the provided content to answer the question.\\\n",
    "                    Do not make assumptions or use information from your pre-trained data. If the answer is not explicitly mentioned in the provided information, \\\n",
    "                    reply with 'unknown'. Use this information: {db_page_contents}\"\"\"})\n",
    "\n",
    "    # Input User's question\n",
    "    context.append({'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"})\n",
    "    response = get_completion_from_messages(context).lower()\n",
    "    context.append({'role': 'assistant', 'content': f\"{response}\"})\n",
    "\n",
    "    # BLEU SCORE\n",
    "    response = remove_punctuation(response)\n",
    "    tokenized_response = get_tokenized_responses(response)\n",
    "    tokenized_actual = get_tokenized_responses(remove_punctuation(row['answer']))\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    # CITATION: Had to change this line of code to work with small answers. Help from ChatGPT.\n",
    "    bleu_score = sentence_bleu([tokenized_response], tokenized_actual, weights=(0.5, 0.5), smoothing_function=smooth_fn)\n",
    "    bleu_scores_array.append(bleu_score)\n",
    "\n",
    "    # METEOR SCORE\n",
    "    meteor_scored = meteor_score([tokenized_response], tokenized_actual)\n",
    "    meteor_scores_array.append(meteor_scored)\n",
    "\n",
    "\n",
    "# print(bleu_scores_array, meteor_scores_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4c0775-d8a2-48c8-8fa1-ebea192e2910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bill of rights\n",
      "['the', 'bill', 'of', 'rights']\n",
      "\n",
      "bill of rights\n",
      "['bill', 'of', 'rights']\n",
      "0.7165313105737893\n"
     ]
    }
   ],
   "source": [
    "# response = remove_punctuation(response)\n",
    "\n",
    "# tokenized_response = get_tokenized_responses(response)\n",
    "# tokenized_actual = get_tokenized_responses(test_df.iloc[0]['answer'])\n",
    "# print(response)\n",
    "# print(tokenized_response)\n",
    "# print()\n",
    "# print(test_df.iloc[0]['answer'])\n",
    "# print(tokenized_actual)\n",
    "\n",
    "# smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "# # CITATION: Had to change this line of code to work with small answers. Help from ChatGPT.\n",
    "# bleu_score = sentence_bleu([tokenized_response], tokenized_actual, weights=(0.5, 0.5), smoothing_function=smooth_fn)\n",
    "\n",
    "# print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "865cab0b-b3fd-4a1f-9798-1e78295edae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'bill', 'of', 'rights']]\n",
      "['bill', 'of', 'rights']\n",
      "0.754985754985755\n"
     ]
    }
   ],
   "source": [
    "# print([tokenized_response])\n",
    "# print(tokenized_actual)\n",
    "\n",
    "# meteor_scores = meteor_score([tokenized_response], tokenized_actual)\n",
    "# print(meteor_scores)\n",
    "\n",
    "# # average_meteor_score = sum(meteor_scores) / len(meteor_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb368f-4002-4728-a6cd-d7bfab3e517e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a31bf7-d3f8-4a31-b133-7068b1166454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - Change script to test on the test AmbigNQ data\n",
    "# - Prompt-engineer GPT model to output short answers.\n",
    "# - Extract each ChatGPT output and Compare answers using embedding model and thus euclidean similarity?\n",
    "\n",
    "# Potential Improvements\n",
    "# - Hallucination tracking\n",
    "# - RAG system with GoogleNQ dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988538a-b781-4fea-831d-c46630bba684",
   "metadata": {},
   "source": [
    "## Chatbot With Ambiguous Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc1a37-741b-4545-8c8b-f93e879e4a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c69975-3c55-4c06-9774-4e56ea1a8079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
